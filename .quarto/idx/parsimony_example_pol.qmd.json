{"title":"Parsimonuous models are robust to context variation","markdown":{"yaml":{"title":"Parsimonuous models are robust to context variation","subtitle":"A simple illustrative example"},"headingText":"The protocol","containsRefs":false,"markdown":"\n\n***\n\nIn this section, we consider a very simple example of the comparison between the behavior of a parsimonious model and several non parsimonious models in order to highlight the robustness of the normality characterization via parsimonious relationships to unseen contexts. \n\n\nWe shall define a dataset that involves $n_x=3$ features through a label that is an exact polynomial of degree `deg`=3 involving only $4$ monomials. \n\n### Training data\n\nThe training dataset is obtained by generating `nSamples`=10000 samples of features in the hypercube: \n\n$$\n\\mathbb X_\\text{train} := [-1,1]^3\n$$\n\nThe label is computed using the polynomial defined by the following powers and coefficients matrix and vector respectively[^seepol]: \n\n$$\nP := \\begin{bmatrix} \n2&1&0\\cr \n2&0&1\\cr \n1&0&0\\cr \n1&1&1\n\\end{bmatrix} \\quad ;\\quad c\\in [-1,1]^4\n$$\n\nwhich leads to a relationships of the form: \n\n$$\ny = c_1x_1^2x_2 + c_2x_1^2x_3 + c_3x_1 + c_4x_1x_2x_3\n$${#eq-les4monomials}\n\nThe label vector, denoted by $y_\\texttt{train}$, associated to the matrix $X_\\text{train}\\in \\mathbb R^{\\texttt{nSamples}\\times 3}$ is obtained by computing the value of the polynomial defined by $(P,c)$ at the 10000 samples (rows of X):\n\n```python\ny_train = poly_func(X_train) + np.random.normal(0, noise_level, size=nSamples)\n```\n\nwhere `noise_level` is taked either equal to 0 or to 0.025 in order to check the robustness to noise of the fitted model. \n\n### The compared models \n\nFive models are fitted using the pair of features matrix  and label defined by $(X_\\texttt{train},y_\\texttt{train})$. Namely: \n\n- **Three Dnn models** with different structures as shown in the following excerpt of the python code: \n\n    ```python\n    lesDnn = [(8,8,2), (32,16,4), (128,128,32)]\n    model_dnn = {i:None for i in range(len(lesDnn))}\n    cpu = {i:None for i in range(len(lesDnn))}\n\n    iModel = 0\n    for iModel, ns in enumerate(lesDnn):\n        n1, n2, n3 = ns\n        model_dnn[iModel] = keras.Sequential()\n        model_dnn[iModel].add(layers.Dense(n2, activation='relu'))\n        model_dnn[iModel].add(layers.Dense(n3, activation='relu'))\n        model_dnn[iModel].add(layers.Dense(n1, activation='relu', input_shape=(nx,)))\n        model_dnn[iModel].add(layers.Dense(1))\n        model_dnn[iModel].compile(optimizer='adam', loss='mse')\n        t0 = time()\n        model_dnn[iModel].fit(Xtrain, ytrain, epochs=500, batch_size=32, validation_split=0.2, verbose=0)\n    ```\n    These models involve an increasing number of neurons in order to show that the qualitative conclusion does not significantly depend on the structure. \n\n\n:::{.callout-tip title=\"Use of validation subset\"}\nNotice that following the good practice, the depth of the fit process is supervised though the use of a **validation set** which is here taked to be 20% of the training set in order to avoid the over-fitting phenomenon. \n:::\n\n\n\n- **One model** based on a **Non parsimonious** linear regression applied to the polynomial features generated from $X_\\text{train}$ as it is shown in the following excerpt: \n\n    ```python \n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeCV\n    from sklearn.pipeline import make_pipeline  \n    model_ridge = make_pipeline(PolynomialFeatures(degree=deg), RidgeCV())\n    t0 = time()\n    model_ridge.fit(Xtrain, ytrain)\n    cpu_ridge = time() - t0\n    ```\n\n- A last model that is obtained using the [`plars` module](plars_intro.qmd) of the `MizoPol` package which seeks a **parsimonious** multivariate polynomial: \n\n    ```python\n    from plars import PLARS, predict, normalized_error\n    pl = PLARS(window=2000, deg=deg, nModels=20, nModes=20, eps=0.05)\n    t0 = time()\n    sol = pl.fit(Xtrain, ytrain)\n    cpu_pl = time() - t0\n    ```\n\n### The test dataset\n\nThe test dataset is defined so that **unseen regions** in the training data are used in order to check the robustness of the normality characterization via the invariant relationships. In other words, we want to answer the question:\n\n*Does the relationship holds on the unseen regions that were absent from the training data?*\n\nFor this reason, a new set of `nSamples=10000` samples are randomly generated inside the hyper-cube:\n\n$$\n\\mathbb X_\\text{test} := [-5,5]^3\n$$\n\nso that many samples lie outside the training set domain $\\mathbb X_\\text{train}=[-1,1]^3$. \n\nThis might be viewed as an instantiation of a new context in which the features visit regions that were not visited in the training dataset while the relationship is kept unchanged. Good models should not see any reason to raise alarms.  \n\n## Results & comparison\n\n### Extrapolation errors\n\nThe statistics on the extrapolation error (residual on the sample in the test dataset) are expressed in terms of normalized percentile, namely:\n\n![**Regression error**: (For instance, $e_y=0.1$ means that 95\\% of the errors are 10 times lower than the median of the norm of $y$).](images/ey.png){width=\"65%\"}\n\n:::{.panel-tabset}\n\n# Normalized error's | noise = 0\n\n![](images/parsim_df_noise0_reorderTrue_zoom.png)\n\n# Normalized error's | noise = 0.025\n\n![](images/parsim_df_noise0_025_reorderTrue_zoom.png)\n\n:::\n\nObviously, the above table show how bad is the generalization power of the DNN-based models. The non sparse solver remains quite good in the absence of noise and starts to seriously be affected as soon measurement noise is added. On the contrary, the sparse model resists in all circumstances. \n\n### Residual vs regions \n\nIn order to better see how the errors spread in the training and the unseen domain, the following plots shows the errors as function of the $L_\\infty$ norm of the features vector. \n\n:::{.panel-tabset}\n\n# noise = 0|all  \n\n![](images/parsim_Error_noise0_reorder_True.png)\n\n# noise = 0|zoom\n\n![](images/parsim_Error_noise0_reorder_True_zoom.png)\n\n# noise = 0.025|all  \n\n![](images/parsim_Error_noise0_025_reorder_True.png)\n\n# noise = 0.025|zoom\n\n![](images/parsim_Error_noise0_025_reorder_True_zoom.png)\n\n:::\n\nThese results clearly show that the DNN models does not resist to unseen contexts in the training data and extrapolate **very badly** on these regions. \n\nOn the other hand, for both noise level, the parsimonious model extrapolates better than the full polynomial-Ridge, **in particular in the presence of noise**. This is because even with this small noise, the full model is slightly detuned because of the availability to fit, even a tiny part of the noise, while the parsimonious model does not possess this *abusive* power. \n\n### Model's cardinality \n\nThis section looks at the number of active coefficients in each of the models discussed above. Only one representative of the set of Dnn models is shown in order to underline that these models involved tens of thousands of non zero coefficients. \n\nAs for the Ridge-polynomial and the `plars` models, infinitely less coefficients are used and in particular, in the case of noise-free training, the exact four coefficients are found which fully explains why this `plars`**-associated model keeps its quality far away from the training region which makes it context-independent**. \n\n:::{.panel-tabset}\n\n# Coefficients of typical Dnn model\n\n![](images/parsim_coefs_Dnn.png)\n\n# ridge-polynomial \n\n![](images/parsim_coefs_ridge.png){width=70%}\n\nNotice that the most important features are those shared with the plars solution and which correspond to the true four monomials being used in @eq-les4monomials. Nevertheless, the presence of the other coefficients leads to higher distorsion outside the training domain $\\mathbb X_\\texttt{train}$. \n\n# plars  \n\n![](images/parsim_coefs_plars.png){width=70%}\n\n:::\n\n[^seepol]: See the section on the definition of [multivariate polynomials](polynomials.qmd).\n\nIn the next section, we dig a little bit deeper inside an equipment in order to understand what are the different reasons that induce ambiguity in the diagnosis of normality. ","srcMarkdownNoYaml":"\n\n***\n\nIn this section, we consider a very simple example of the comparison between the behavior of a parsimonious model and several non parsimonious models in order to highlight the robustness of the normality characterization via parsimonious relationships to unseen contexts. \n\n## The protocol \n\nWe shall define a dataset that involves $n_x=3$ features through a label that is an exact polynomial of degree `deg`=3 involving only $4$ monomials. \n\n### Training data\n\nThe training dataset is obtained by generating `nSamples`=10000 samples of features in the hypercube: \n\n$$\n\\mathbb X_\\text{train} := [-1,1]^3\n$$\n\nThe label is computed using the polynomial defined by the following powers and coefficients matrix and vector respectively[^seepol]: \n\n$$\nP := \\begin{bmatrix} \n2&1&0\\cr \n2&0&1\\cr \n1&0&0\\cr \n1&1&1\n\\end{bmatrix} \\quad ;\\quad c\\in [-1,1]^4\n$$\n\nwhich leads to a relationships of the form: \n\n$$\ny = c_1x_1^2x_2 + c_2x_1^2x_3 + c_3x_1 + c_4x_1x_2x_3\n$${#eq-les4monomials}\n\nThe label vector, denoted by $y_\\texttt{train}$, associated to the matrix $X_\\text{train}\\in \\mathbb R^{\\texttt{nSamples}\\times 3}$ is obtained by computing the value of the polynomial defined by $(P,c)$ at the 10000 samples (rows of X):\n\n```python\ny_train = poly_func(X_train) + np.random.normal(0, noise_level, size=nSamples)\n```\n\nwhere `noise_level` is taked either equal to 0 or to 0.025 in order to check the robustness to noise of the fitted model. \n\n### The compared models \n\nFive models are fitted using the pair of features matrix  and label defined by $(X_\\texttt{train},y_\\texttt{train})$. Namely: \n\n- **Three Dnn models** with different structures as shown in the following excerpt of the python code: \n\n    ```python\n    lesDnn = [(8,8,2), (32,16,4), (128,128,32)]\n    model_dnn = {i:None for i in range(len(lesDnn))}\n    cpu = {i:None for i in range(len(lesDnn))}\n\n    iModel = 0\n    for iModel, ns in enumerate(lesDnn):\n        n1, n2, n3 = ns\n        model_dnn[iModel] = keras.Sequential()\n        model_dnn[iModel].add(layers.Dense(n2, activation='relu'))\n        model_dnn[iModel].add(layers.Dense(n3, activation='relu'))\n        model_dnn[iModel].add(layers.Dense(n1, activation='relu', input_shape=(nx,)))\n        model_dnn[iModel].add(layers.Dense(1))\n        model_dnn[iModel].compile(optimizer='adam', loss='mse')\n        t0 = time()\n        model_dnn[iModel].fit(Xtrain, ytrain, epochs=500, batch_size=32, validation_split=0.2, verbose=0)\n    ```\n    These models involve an increasing number of neurons in order to show that the qualitative conclusion does not significantly depend on the structure. \n\n\n:::{.callout-tip title=\"Use of validation subset\"}\nNotice that following the good practice, the depth of the fit process is supervised though the use of a **validation set** which is here taked to be 20% of the training set in order to avoid the over-fitting phenomenon. \n:::\n\n\n\n- **One model** based on a **Non parsimonious** linear regression applied to the polynomial features generated from $X_\\text{train}$ as it is shown in the following excerpt: \n\n    ```python \n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import RidgeCV\n    from sklearn.pipeline import make_pipeline  \n    model_ridge = make_pipeline(PolynomialFeatures(degree=deg), RidgeCV())\n    t0 = time()\n    model_ridge.fit(Xtrain, ytrain)\n    cpu_ridge = time() - t0\n    ```\n\n- A last model that is obtained using the [`plars` module](plars_intro.qmd) of the `MizoPol` package which seeks a **parsimonious** multivariate polynomial: \n\n    ```python\n    from plars import PLARS, predict, normalized_error\n    pl = PLARS(window=2000, deg=deg, nModels=20, nModes=20, eps=0.05)\n    t0 = time()\n    sol = pl.fit(Xtrain, ytrain)\n    cpu_pl = time() - t0\n    ```\n\n### The test dataset\n\nThe test dataset is defined so that **unseen regions** in the training data are used in order to check the robustness of the normality characterization via the invariant relationships. In other words, we want to answer the question:\n\n*Does the relationship holds on the unseen regions that were absent from the training data?*\n\nFor this reason, a new set of `nSamples=10000` samples are randomly generated inside the hyper-cube:\n\n$$\n\\mathbb X_\\text{test} := [-5,5]^3\n$$\n\nso that many samples lie outside the training set domain $\\mathbb X_\\text{train}=[-1,1]^3$. \n\nThis might be viewed as an instantiation of a new context in which the features visit regions that were not visited in the training dataset while the relationship is kept unchanged. Good models should not see any reason to raise alarms.  \n\n## Results & comparison\n\n### Extrapolation errors\n\nThe statistics on the extrapolation error (residual on the sample in the test dataset) are expressed in terms of normalized percentile, namely:\n\n![**Regression error**: (For instance, $e_y=0.1$ means that 95\\% of the errors are 10 times lower than the median of the norm of $y$).](images/ey.png){width=\"65%\"}\n\n:::{.panel-tabset}\n\n# Normalized error's | noise = 0\n\n![](images/parsim_df_noise0_reorderTrue_zoom.png)\n\n# Normalized error's | noise = 0.025\n\n![](images/parsim_df_noise0_025_reorderTrue_zoom.png)\n\n:::\n\nObviously, the above table show how bad is the generalization power of the DNN-based models. The non sparse solver remains quite good in the absence of noise and starts to seriously be affected as soon measurement noise is added. On the contrary, the sparse model resists in all circumstances. \n\n### Residual vs regions \n\nIn order to better see how the errors spread in the training and the unseen domain, the following plots shows the errors as function of the $L_\\infty$ norm of the features vector. \n\n:::{.panel-tabset}\n\n# noise = 0|all  \n\n![](images/parsim_Error_noise0_reorder_True.png)\n\n# noise = 0|zoom\n\n![](images/parsim_Error_noise0_reorder_True_zoom.png)\n\n# noise = 0.025|all  \n\n![](images/parsim_Error_noise0_025_reorder_True.png)\n\n# noise = 0.025|zoom\n\n![](images/parsim_Error_noise0_025_reorder_True_zoom.png)\n\n:::\n\nThese results clearly show that the DNN models does not resist to unseen contexts in the training data and extrapolate **very badly** on these regions. \n\nOn the other hand, for both noise level, the parsimonious model extrapolates better than the full polynomial-Ridge, **in particular in the presence of noise**. This is because even with this small noise, the full model is slightly detuned because of the availability to fit, even a tiny part of the noise, while the parsimonious model does not possess this *abusive* power. \n\n### Model's cardinality \n\nThis section looks at the number of active coefficients in each of the models discussed above. Only one representative of the set of Dnn models is shown in order to underline that these models involved tens of thousands of non zero coefficients. \n\nAs for the Ridge-polynomial and the `plars` models, infinitely less coefficients are used and in particular, in the case of noise-free training, the exact four coefficients are found which fully explains why this `plars`**-associated model keeps its quality far away from the training region which makes it context-independent**. \n\n:::{.panel-tabset}\n\n# Coefficients of typical Dnn model\n\n![](images/parsim_coefs_Dnn.png)\n\n# ridge-polynomial \n\n![](images/parsim_coefs_ridge.png){width=70%}\n\nNotice that the most important features are those shared with the plars solution and which correspond to the true four monomials being used in @eq-les4monomials. Nevertheless, the presence of the other coefficients leads to higher distorsion outside the training domain $\\mathbb X_\\texttt{train}$. \n\n# plars  \n\n![](images/parsim_coefs_plars.png){width=70%}\n\n:::\n\n[^seepol]: See the section on the definition of [multivariate polynomials](polynomials.qmd).\n\nIn the next section, we dig a little bit deeper inside an equipment in order to understand what are the different reasons that induce ambiguity in the diagnosis of normality. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":"mathjax","css":["styles/styles.css"],"toc":true,"output-file":"parsimony_example_pol.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","bibliography":["biblio.bib"],"resources":["images/sparsity_osc_*.png","images/anomalies_detection_pbstat_*.png","images/sources_of_anomalies_*.png"],"theme":"flatly","link-citations":true,"csl":"styles/apa.csl","title":"Parsimonuous models are robust to context variation","subtitle":"A simple illustrative example"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}