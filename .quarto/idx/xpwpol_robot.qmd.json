{"title":"xpwpol","markdown":{"yaml":{"title":"xpwpol","subtitle":"Explicit piece-wise polynomial relationship for an industrial robot."},"headingText":"Dataset","containsRefs":false,"markdown":"\n\nIn this section, we consider the same robot invoked in the [plars use-case](plars_staubli_4.qmd) where we used a single multivariate polynomial to capture the realtionship between the torque and the kinematic features of a 4-axis manipulator robot. \n\nIn this section, comparisons are provided, in terms of the prediction error, between the explicit piece-wise polynomial structure and  the following two modeling alternatives: \n\n- the `plars` module providing an approximation using a single multivariate polynomial in the features vector. \n\n- the DNN-GRU structure as presented in @ALAMIRstaubli2025 \n\n\nThese comparisons are provided in order to assess the relevance of the piece-wise polynomial strucutre handled by the `xpwpol` module. \n\nFor the sake of easiness of reading, we repeat some of the information given in the above cited section. \n\n***\n\n![The manipulator robot with 4-axis](images/staubliPhoto.png){width=\"20%\" fig-align=\"left\"}\n\n\n*** \n\n\nWe have the following dataframe representing the recording of kinematic variables and the torques at the axis joints:\n\n$$\n\\{q_i\\}_{i=1}^4,\\quad \\{\\dot q_i\\}_{i=1}^4, \\quad\\{\\ddot q_i\\}_{i=1}^4,\\quad \\{T_i\\}_{i=1}^4\n$$\n \nof a 4-axis manipulator robot. \n\n\n![Head of the dataframe containing 1617936 rows and 16 columns](images/staubli4dataframe.png)\n\n\n## Problem statement \n\n:::{.callout-note title=\"The regression problem\"}\n\nFind a sparse **explicit** piece-wise polynomial relationship of the form given by @eq-regprob, that provides a prediction of the torque applied to axis $i\\in \\{1,\\dots,4\\}$:\n$$\nT_i \\approx P_{\\mathcal R(x)}\\Bigl(x\\Bigr)\\quad \\vert\\quad x:= \\begin{bmatrix} \nq\\cr \\dot q\\cr \\ddot q\n\\end{bmatrix}\\in \\mathbb R^{12}\n$${#eq-regprob}\nwhere $\\mathcal R(x): \\mathbb R^{12}\\rightarrow \\{1,\\dots, n_r\\}$ is a region map associating to each value of the  vector of features $x$, a region number. \n\n$n_r$ stands for the number of regions (and hence polynomials) involved in the explicit piece-wise solution.  \n:::\n\n## Scripts & results\n\nFor illustration purposes, the results concerning axis 3 and 4 are given with comparison to the `plars` and the DNN-GRU models. \n\n### Results for axis 4\n\n:::{.panel-tabset}\n\n\n## Data preparation \n\n```python \n\n# Choose the axis and the split ratio \naxe = 4\ntest_size=0.8\n\n# Compute Xtrain, Xtest, ytrain, ytest\nX = df[colX].values\ny = df[colY[axe-1]].values\n\nnTrain = int(len(X)*(1-test_size))\nXtrain = X[0:nTrain]\nytrain = y[0:nTrain]\nXtest = X[nTrain:]\nytest = y[nTrain:]\n```\n\n## Fit the model \n\n```python \n# Parameter helping the design of the different regions \nnDiv = np.array([2,2,2,2])\n\n# Create an instance of the class Xpwpol\n# Notice the similarity with plars instantiation\n\nxpwpol = Xpwpol(window=2000, \n                deg=2, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\n# Fit the solution \nxpwpol.fit(Xtrain, ytrain)\n\n 1/16 |  0 |0.55 |ncoef =     10 | nrows =  21496 |avg-error = 0.55 | \n 2/16 |  1 |0.38 |ncoef =     16 | nrows =  16608 |avg-error = 0.47 | \n 3/16 |  2 |0.54 |ncoef =     29 | nrows =  18498 |avg-error = 0.50 | \n 4/16 |  3 |0.35 |ncoef =     38 | nrows =  19911 |avg-error = 0.46 | \n 5/16 |  4 |0.52 |ncoef =     50 | nrows =  18854 |avg-error = 0.47 | \n 6/16 |  5 |0.39 |ncoef =     64 | nrows =  19002 |avg-error = 0.46 | \n 7/16 |  6 |0.58 |ncoef =     77 | nrows =  21222 |avg-error = 0.48 | \n 8/16 |  7 |0.33 |ncoef =     82 | nrows =  26202 |avg-error = 0.45 | \n 9/16 |  8 |0.58 |ncoef =     89 | nrows =  26642 |avg-error = 0.47 | \n10/16 |  9 |0.55 |ncoef =    106 | nrows =  20341 |avg-error = 0.48 | \n11/16 | 10 |0.64 |ncoef =    116 | nrows =  17897 |avg-error = 0.49 | \n12/16 | 11 |0.39 |ncoef =    124 | nrows =  20400 |avg-error = 0.48 | \n13/16 | 12 |0.60 |ncoef =    131 | nrows =  20820 |avg-error = 0.49 | \n14/16 | 13 |0.44 |ncoef =    141 | nrows =  18030 |avg-error = 0.49 | \n15/16 | 14 |0.73 |ncoef =    150 | nrows =  16364 |avg-error = 0.50 | \n16/16 | 15 |0.40 |ncoef =    160 | nrows =  21300 |avg-error = 0.49 | \n```\n\n## Error statistics\n\n```python \nfrom plars import normalized_error\n\nypred = xpwpol.predict(Xtest)\ndfTest = normalized_error(ytest, ypred)\n\nypred = xpwpol.predict(Xtrain)\ndfTrain = normalized_error(ytrain, ypred)\n\ndfResult = pd.concat([dfTrain, dfTest], axis=1)\ndfResult.columns = ['Err-Ttrain', 'Err-Test']\ndfResult\n\n\tErr-Ttrain\tErr-Test\n50%\t0.132826\t0.133281\n80%\t0.267617\t0.269011\n90%\t0.366049\t0.367497\n95%\t0.474345\t0.476682\n98%\t0.673459\t0.675066\n99%\t0.884998\t0.888047\n```\n\n## Comparisons\n\n```python \npl = PLARS(window=2000, deg=3, nModels=20, nModes=20, eps=0.05)\nnTrain = int(len(X)*(1-test_size))\nsol = pl.fit(Xtrain, ytrain)\n\nyhat_pol_train = predict(Xtrain, sol)\ndfplars_train = normalized_error(ytrain, yhat_pol_train)\n\nyhat_pol_test = predict(Xtest, sol)\ndfplars_test = normalized_error(ytest, yhat_pol_test)\n\n\ndfResult = pd.concat([dfResult, dfplars_train, dfplars_test], axis=1)\ndfResult.columns = ['Err-Ttrain|xpwpol', \n                    'Err-Test|xpwpol', \n                    'Err-plars on train', \n                    'Err-plars on test']\ndfResult\n```\n\n![Axis 4: Comparison of the statistics of errors between the models provided by the `plars` and the `xpwpol` module of the MizoPol package.](images/comparisonxplars.png)\n\nAs for the statistics of the DNN-GRU model (@ALAMIRstaubli2025), they are given in the figures below: \n\n![Statistics of the error when using DNN-GRU strucutre with a hidden layer of 32 or 128 correponding respectively to **31224** and **235128** weights and **computation time of respectively 3h27 and 3h40**.](images/results_axis4_DNNGRU.png)\n\nNotice that the number of parameters used by the **xpwpol** module is equal to **160** parameters and the computation time is less than 60 sec. \n\n:::{.callout-note title='To summarize'}\nThe precision of the model provided by `xpwpol` is largely better than the one provided by the `plars` and is slightly better than the one provided by the DNN-GRU model while the computation time is totally out of comparison. \n\nMoreover, the number of parameters involved clearly shows that we are in the presence of sparse and hence robust solution. \n::: \n\n:::\n\n### Results for axis 3\n\n:::{.panel-tabset}\n\n\n## Data preparation \n\n```python \n\n# Choose the axis and the split ratio \naxe = 3\ntest_size=0.25\n\n# Compute Xtrain, Xtest, ytrain, ytest\nX = df[colX].values\ny = df[colY[axe-1]].values\n\nnTrain = int(len(X)*(1-test_size))\nXtrain = X[0:nTrain]\nytrain = y[0:nTrain]\nXtest = X[nTrain:]\nytest = y[nTrain:]\n```\n\n## Fit the model \n\n```python \n# Parameter helping the design of the different regions \nnDiv = np.array([2,2,2,2])\n\n# Create an instance of the class Xpwpol\n# Notice the similarity with plars instantiation\n\nxpwpol = Xpwpol(window=2000, \n                deg=4, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\n# Fit the solution \nxpwpol.fit(Xtrain, ytrain)\n\n from xpwpol import Xpwpol\n\nnDiv = np.array([2,2,2,2])\n\nxpwpol = Xpwpol(window=2000, \n                deg=4, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\nxpwpol.fit(Xtrain, \n           ytrain, \n           compute_contributions=True, \n           colNames=colX)\n\n 1/16 |  0 |0.36 |ncoef =    243 | nrows =  73252 |avg-error = 0.36 | \n 2/16 |  1 |0.90 |ncoef =    505 | nrows =  65127 |avg-error = 0.62 | \n 3/16 |  2 |0.38 |ncoef =    739 | nrows =  76105 |avg-error = 0.53 | \n 4/16 |  3 |0.89 |ncoef =    988 | nrows =  81846 |avg-error = 0.63 | \n 5/16 |  4 |0.37 |ncoef =   1230 | nrows =  75433 |avg-error = 0.58 | \n 6/16 |  5 |0.86 |ncoef =   1472 | nrows =  77318 |avg-error = 0.63 | \n 7/16 |  6 |0.38 |ncoef =   1726 | nrows =  74982 |avg-error = 0.59 | \n 8/16 |  7 |0.89 |ncoef =   1975 | nrows =  82663 |avg-error = 0.63 | \n 9/16 |  8 |0.39 |ncoef =   2236 | nrows =  84477 |avg-error = 0.60 | \n10/16 |  9 |0.84 |ncoef =   2487 | nrows =  73981 |avg-error = 0.62 | \n11/16 | 10 |0.40 |ncoef =   2746 | nrows =  73550 |avg-error = 0.60 | \n12/16 | 11 |0.90 |ncoef =   2993 | nrows =  78388 |avg-error = 0.63 | \n13/16 | 12 |0.39 |ncoef =   3249 | nrows =  82354 |avg-error = 0.61 | \n14/16 | 13 |0.91 |ncoef =   3486 | nrows =  74784 |avg-error = 0.63 | \n15/16 | 14 |0.42 |ncoef =   3722 | nrows =  66573 |avg-error = 0.62 | \n16/16 | 15 |0.92 |ncoef =   3979 | nrows =  72619 |avg-error = 0.64 | \n```\n\n## Error statistics\n\n```python \nfrom plars import normalized_error\n\nypred = xpwpol.predict(Xtest)\ndfTest = normalized_error(ytest, ypred)\n\nypred = xpwpol.predict(Xtrain)\ndfTrain = normalized_error(ytrain, ypred)\n\ndfResult = pd.concat([dfTrain, dfTest], axis=1)\ndfResult.columns = ['Err-Ttrain', 'Err-Test']\ndfResult\n\n\tErr-Ttrain\tErr-Test\n50%\t0.179445\t0.189401\n80%\t0.345648\t0.359500\n90%\t0.443848\t0.457406\n95%\t0.523262\t0.540844\n98%\t0.611879\t0.648598\n99%\t0.674233\t0.755522\n```\n\n## Comparisons\n\n```python \npl = PLARS(window=2000, deg=4, nModels=20, nModes=20, eps=0.05)\nnTrain = int(len(X)*(1-test_size))\nsol = pl.fit(Xtrain, ytrain)\n\nyhat_pol_train = predict(Xtrain, sol)\ndfplars_train = normalized_error(ytrain, yhat_pol_train)\n\nyhat_pol_test = predict(Xtest, sol)\ndfplars_test = normalized_error(ytest, yhat_pol_test)\n\n\ndfResult = pd.concat([dfResult, dfplars_train, dfplars_test], axis=1)\ndfResult.columns = ['Err-Ttrain|xpwpol', \n                    'Err-Test|xpwpol', \n                    'Err-plars on train', \n                    'Err-plars on test']\ndfResult\n```\n\n![Axis 3: Comparison of the error's statistics between the models provided by the `plars` and the `xpwpol` module of the MizoPol package.](images/comparisonlarsaxis3.png)\n\nAs for the statistics of the DNN-GRU model (@ALAMIRstaubli2025), they are given in the figures below: \n\n![Statistics of the error when using DNN-GRU strucutre with a hidden layer of 32 or 128 correponding respectively to **31224** and **235128** weights and **computation time of respectively 3h27 and 3h40**.](images/results_axis3_DNNGRU.png)\n\nNotice that the number of parameters used by the **xpwpol** module is around **4000** parameters and the computation time is less than 180 sec. \n\n:::\n\n","srcMarkdownNoYaml":"\n\nIn this section, we consider the same robot invoked in the [plars use-case](plars_staubli_4.qmd) where we used a single multivariate polynomial to capture the realtionship between the torque and the kinematic features of a 4-axis manipulator robot. \n\nIn this section, comparisons are provided, in terms of the prediction error, between the explicit piece-wise polynomial structure and  the following two modeling alternatives: \n\n- the `plars` module providing an approximation using a single multivariate polynomial in the features vector. \n\n- the DNN-GRU structure as presented in @ALAMIRstaubli2025 \n\n\nThese comparisons are provided in order to assess the relevance of the piece-wise polynomial strucutre handled by the `xpwpol` module. \n\nFor the sake of easiness of reading, we repeat some of the information given in the above cited section. \n\n***\n\n![The manipulator robot with 4-axis](images/staubliPhoto.png){width=\"20%\" fig-align=\"left\"}\n\n\n*** \n\n## Dataset \n\nWe have the following dataframe representing the recording of kinematic variables and the torques at the axis joints:\n\n$$\n\\{q_i\\}_{i=1}^4,\\quad \\{\\dot q_i\\}_{i=1}^4, \\quad\\{\\ddot q_i\\}_{i=1}^4,\\quad \\{T_i\\}_{i=1}^4\n$$\n \nof a 4-axis manipulator robot. \n\n\n![Head of the dataframe containing 1617936 rows and 16 columns](images/staubli4dataframe.png)\n\n\n## Problem statement \n\n:::{.callout-note title=\"The regression problem\"}\n\nFind a sparse **explicit** piece-wise polynomial relationship of the form given by @eq-regprob, that provides a prediction of the torque applied to axis $i\\in \\{1,\\dots,4\\}$:\n$$\nT_i \\approx P_{\\mathcal R(x)}\\Bigl(x\\Bigr)\\quad \\vert\\quad x:= \\begin{bmatrix} \nq\\cr \\dot q\\cr \\ddot q\n\\end{bmatrix}\\in \\mathbb R^{12}\n$${#eq-regprob}\nwhere $\\mathcal R(x): \\mathbb R^{12}\\rightarrow \\{1,\\dots, n_r\\}$ is a region map associating to each value of the  vector of features $x$, a region number. \n\n$n_r$ stands for the number of regions (and hence polynomials) involved in the explicit piece-wise solution.  \n:::\n\n## Scripts & results\n\nFor illustration purposes, the results concerning axis 3 and 4 are given with comparison to the `plars` and the DNN-GRU models. \n\n### Results for axis 4\n\n:::{.panel-tabset}\n\n\n## Data preparation \n\n```python \n\n# Choose the axis and the split ratio \naxe = 4\ntest_size=0.8\n\n# Compute Xtrain, Xtest, ytrain, ytest\nX = df[colX].values\ny = df[colY[axe-1]].values\n\nnTrain = int(len(X)*(1-test_size))\nXtrain = X[0:nTrain]\nytrain = y[0:nTrain]\nXtest = X[nTrain:]\nytest = y[nTrain:]\n```\n\n## Fit the model \n\n```python \n# Parameter helping the design of the different regions \nnDiv = np.array([2,2,2,2])\n\n# Create an instance of the class Xpwpol\n# Notice the similarity with plars instantiation\n\nxpwpol = Xpwpol(window=2000, \n                deg=2, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\n# Fit the solution \nxpwpol.fit(Xtrain, ytrain)\n\n 1/16 |  0 |0.55 |ncoef =     10 | nrows =  21496 |avg-error = 0.55 | \n 2/16 |  1 |0.38 |ncoef =     16 | nrows =  16608 |avg-error = 0.47 | \n 3/16 |  2 |0.54 |ncoef =     29 | nrows =  18498 |avg-error = 0.50 | \n 4/16 |  3 |0.35 |ncoef =     38 | nrows =  19911 |avg-error = 0.46 | \n 5/16 |  4 |0.52 |ncoef =     50 | nrows =  18854 |avg-error = 0.47 | \n 6/16 |  5 |0.39 |ncoef =     64 | nrows =  19002 |avg-error = 0.46 | \n 7/16 |  6 |0.58 |ncoef =     77 | nrows =  21222 |avg-error = 0.48 | \n 8/16 |  7 |0.33 |ncoef =     82 | nrows =  26202 |avg-error = 0.45 | \n 9/16 |  8 |0.58 |ncoef =     89 | nrows =  26642 |avg-error = 0.47 | \n10/16 |  9 |0.55 |ncoef =    106 | nrows =  20341 |avg-error = 0.48 | \n11/16 | 10 |0.64 |ncoef =    116 | nrows =  17897 |avg-error = 0.49 | \n12/16 | 11 |0.39 |ncoef =    124 | nrows =  20400 |avg-error = 0.48 | \n13/16 | 12 |0.60 |ncoef =    131 | nrows =  20820 |avg-error = 0.49 | \n14/16 | 13 |0.44 |ncoef =    141 | nrows =  18030 |avg-error = 0.49 | \n15/16 | 14 |0.73 |ncoef =    150 | nrows =  16364 |avg-error = 0.50 | \n16/16 | 15 |0.40 |ncoef =    160 | nrows =  21300 |avg-error = 0.49 | \n```\n\n## Error statistics\n\n```python \nfrom plars import normalized_error\n\nypred = xpwpol.predict(Xtest)\ndfTest = normalized_error(ytest, ypred)\n\nypred = xpwpol.predict(Xtrain)\ndfTrain = normalized_error(ytrain, ypred)\n\ndfResult = pd.concat([dfTrain, dfTest], axis=1)\ndfResult.columns = ['Err-Ttrain', 'Err-Test']\ndfResult\n\n\tErr-Ttrain\tErr-Test\n50%\t0.132826\t0.133281\n80%\t0.267617\t0.269011\n90%\t0.366049\t0.367497\n95%\t0.474345\t0.476682\n98%\t0.673459\t0.675066\n99%\t0.884998\t0.888047\n```\n\n## Comparisons\n\n```python \npl = PLARS(window=2000, deg=3, nModels=20, nModes=20, eps=0.05)\nnTrain = int(len(X)*(1-test_size))\nsol = pl.fit(Xtrain, ytrain)\n\nyhat_pol_train = predict(Xtrain, sol)\ndfplars_train = normalized_error(ytrain, yhat_pol_train)\n\nyhat_pol_test = predict(Xtest, sol)\ndfplars_test = normalized_error(ytest, yhat_pol_test)\n\n\ndfResult = pd.concat([dfResult, dfplars_train, dfplars_test], axis=1)\ndfResult.columns = ['Err-Ttrain|xpwpol', \n                    'Err-Test|xpwpol', \n                    'Err-plars on train', \n                    'Err-plars on test']\ndfResult\n```\n\n![Axis 4: Comparison of the statistics of errors between the models provided by the `plars` and the `xpwpol` module of the MizoPol package.](images/comparisonxplars.png)\n\nAs for the statistics of the DNN-GRU model (@ALAMIRstaubli2025), they are given in the figures below: \n\n![Statistics of the error when using DNN-GRU strucutre with a hidden layer of 32 or 128 correponding respectively to **31224** and **235128** weights and **computation time of respectively 3h27 and 3h40**.](images/results_axis4_DNNGRU.png)\n\nNotice that the number of parameters used by the **xpwpol** module is equal to **160** parameters and the computation time is less than 60 sec. \n\n:::{.callout-note title='To summarize'}\nThe precision of the model provided by `xpwpol` is largely better than the one provided by the `plars` and is slightly better than the one provided by the DNN-GRU model while the computation time is totally out of comparison. \n\nMoreover, the number of parameters involved clearly shows that we are in the presence of sparse and hence robust solution. \n::: \n\n:::\n\n### Results for axis 3\n\n:::{.panel-tabset}\n\n\n## Data preparation \n\n```python \n\n# Choose the axis and the split ratio \naxe = 3\ntest_size=0.25\n\n# Compute Xtrain, Xtest, ytrain, ytest\nX = df[colX].values\ny = df[colY[axe-1]].values\n\nnTrain = int(len(X)*(1-test_size))\nXtrain = X[0:nTrain]\nytrain = y[0:nTrain]\nXtest = X[nTrain:]\nytest = y[nTrain:]\n```\n\n## Fit the model \n\n```python \n# Parameter helping the design of the different regions \nnDiv = np.array([2,2,2,2])\n\n# Create an instance of the class Xpwpol\n# Notice the similarity with plars instantiation\n\nxpwpol = Xpwpol(window=2000, \n                deg=4, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\n# Fit the solution \nxpwpol.fit(Xtrain, ytrain)\n\n from xpwpol import Xpwpol\n\nnDiv = np.array([2,2,2,2])\n\nxpwpol = Xpwpol(window=2000, \n                deg=4, \n                nModels=20, \n                nModes=20, \n                eps=0.05, \n                nDiv=nDiv)\n\nxpwpol.fit(Xtrain, \n           ytrain, \n           compute_contributions=True, \n           colNames=colX)\n\n 1/16 |  0 |0.36 |ncoef =    243 | nrows =  73252 |avg-error = 0.36 | \n 2/16 |  1 |0.90 |ncoef =    505 | nrows =  65127 |avg-error = 0.62 | \n 3/16 |  2 |0.38 |ncoef =    739 | nrows =  76105 |avg-error = 0.53 | \n 4/16 |  3 |0.89 |ncoef =    988 | nrows =  81846 |avg-error = 0.63 | \n 5/16 |  4 |0.37 |ncoef =   1230 | nrows =  75433 |avg-error = 0.58 | \n 6/16 |  5 |0.86 |ncoef =   1472 | nrows =  77318 |avg-error = 0.63 | \n 7/16 |  6 |0.38 |ncoef =   1726 | nrows =  74982 |avg-error = 0.59 | \n 8/16 |  7 |0.89 |ncoef =   1975 | nrows =  82663 |avg-error = 0.63 | \n 9/16 |  8 |0.39 |ncoef =   2236 | nrows =  84477 |avg-error = 0.60 | \n10/16 |  9 |0.84 |ncoef =   2487 | nrows =  73981 |avg-error = 0.62 | \n11/16 | 10 |0.40 |ncoef =   2746 | nrows =  73550 |avg-error = 0.60 | \n12/16 | 11 |0.90 |ncoef =   2993 | nrows =  78388 |avg-error = 0.63 | \n13/16 | 12 |0.39 |ncoef =   3249 | nrows =  82354 |avg-error = 0.61 | \n14/16 | 13 |0.91 |ncoef =   3486 | nrows =  74784 |avg-error = 0.63 | \n15/16 | 14 |0.42 |ncoef =   3722 | nrows =  66573 |avg-error = 0.62 | \n16/16 | 15 |0.92 |ncoef =   3979 | nrows =  72619 |avg-error = 0.64 | \n```\n\n## Error statistics\n\n```python \nfrom plars import normalized_error\n\nypred = xpwpol.predict(Xtest)\ndfTest = normalized_error(ytest, ypred)\n\nypred = xpwpol.predict(Xtrain)\ndfTrain = normalized_error(ytrain, ypred)\n\ndfResult = pd.concat([dfTrain, dfTest], axis=1)\ndfResult.columns = ['Err-Ttrain', 'Err-Test']\ndfResult\n\n\tErr-Ttrain\tErr-Test\n50%\t0.179445\t0.189401\n80%\t0.345648\t0.359500\n90%\t0.443848\t0.457406\n95%\t0.523262\t0.540844\n98%\t0.611879\t0.648598\n99%\t0.674233\t0.755522\n```\n\n## Comparisons\n\n```python \npl = PLARS(window=2000, deg=4, nModels=20, nModes=20, eps=0.05)\nnTrain = int(len(X)*(1-test_size))\nsol = pl.fit(Xtrain, ytrain)\n\nyhat_pol_train = predict(Xtrain, sol)\ndfplars_train = normalized_error(ytrain, yhat_pol_train)\n\nyhat_pol_test = predict(Xtest, sol)\ndfplars_test = normalized_error(ytest, yhat_pol_test)\n\n\ndfResult = pd.concat([dfResult, dfplars_train, dfplars_test], axis=1)\ndfResult.columns = ['Err-Ttrain|xpwpol', \n                    'Err-Test|xpwpol', \n                    'Err-plars on train', \n                    'Err-plars on test']\ndfResult\n```\n\n![Axis 3: Comparison of the error's statistics between the models provided by the `plars` and the `xpwpol` module of the MizoPol package.](images/comparisonlarsaxis3.png)\n\nAs for the statistics of the DNN-GRU model (@ALAMIRstaubli2025), they are given in the figures below: \n\n![Statistics of the error when using DNN-GRU strucutre with a hidden layer of 32 or 128 correponding respectively to **31224** and **235128** weights and **computation time of respectively 3h27 and 3h40**.](images/results_axis3_DNNGRU.png)\n\nNotice that the number of parameters used by the **xpwpol** module is around **4000** parameters and the computation time is less than 180 sec. \n\n:::\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":"mathjax","css":["styles/styles.css"],"toc":true,"output-file":"xpwpol_robot.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","bibliography":["biblio.bib"],"resources":["images/sparsity_osc_*.png","images/anomalies_detection_pbstat_*.png","images/sources_of_anomalies_*.png","images/zema_*.png"],"theme":"flatly","link-citations":true,"csl":"styles/apa.csl","title":"xpwpol","subtitle":"Explicit piece-wise polynomial relationship for an industrial robot."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}