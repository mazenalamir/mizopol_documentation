{"title":"Parsimony & sparse models","markdown":{"yaml":{"title":"Parsimony & sparse models","subtitle":"Why parsimony is crucial in industrial data-driven solutions?"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n***\n\n\nIn this section, we discuss the importance of **parsimonious models** when addressing the characterization of normality in industrial data, particularly in the framework of normality characterization and anomaly detection. \n\nFirst of all, an informal discussion is proposed before a detailed use-case is shown that explicitly illustrates the relevance of deriving parsimonious models. \n\n:::{.callout-note title=\"Parsimony vs sparsity\"}\nThe words *parsimony* and *sparsity* are intimately linked and this is why sometimes, they are interchageably used. Roughly speaking, a model is described as parsimonious if it is defined through a vector of parameters that is sparse (contains many zeros). \n\nConsequently, one can state that **looking for sparsity**, at the search step,  induces parsimonious models. \n:::\n\n\n## Why sparsity is relevant in industry?\n\n### Anomaly detection \n\nThe parsimony, by helping avoiding *hasardous excursions* when the features go away from the domain spanned by the training data, improves the quality of the detection as it is suggested in the following sketchy representation:\n\n:::{.panel-tabset}\n\n## Parsimonious \n\n![Since the model (in Green) is parsimonious and still fits almost all points, the red one can be declared to be anomalous **with high confidence**.](images/parsimony1){width=80%}\n\n## Non parsimonious\n\n![Because the model (in Green) is not parsimonuous, the probability it overfit the data is greater making the diagnosis of the red circle more difficult to assess with confidence.](images/parsimony2){width=80%}\n:::\n\nThis example shows that: \n\n:::{.callout-tip title=\"Sparsity induces better detectability\"}\nsparsity helps detecting true anomalies compared to *nervous* models that might wrongly visit irrelevant regions inducing false normality bias. \n:::\n\nWhile this simple example highlights the relevance of sparsity in detecting true anomalies, the following discussion focuses on its role in avoinding false alarm upon changes in the context of use. \n\n\nThe need for sparsity in the industrial case stems from two major specificities, namely: The curse of contexts and the aversion to false alarms. This issues are explained hereafter.\n\n### The curse of context\n\nOne of the major obstacles to a correct normality characterization is linked to the fact that: \n\nThe training data scarcely contains all possible contexts\n: Take a robot for instance, the context of use might be represented by the types of trajectories, the types of tasks, the excursion of the kinematic or dynamic variables contained. On all these items, the training data might not represent all the different contexts. \n\n:::{.callout-warning title=\"False alarm on new contexts\"}\nThe multiplicity of contexts of use and their highly probable absence in the training data enhances the risk of raising **false alarm** because of the resurgence of an unseen contexts rather than because of a real change in the system's integrity or health parameters. \n\nIn the [Anomalies vs operational changes](state_context_anomalies.qmd) section, a more detailed visual explanation of the state and context-induced anomalies that need to be distinguished from parameter changes (induced by faults) in dynamical systems. \n:::\n\n### The aversion to false alarms\n\nAnother specificity of industrial applications is that:\n\nIndustry is very sensitive to false alarms\n: Compared to a bad classification in images or an erroneous book recommendation:\n\n:::{.callout-tip title='The cost of False alarm in industry'}\n a false alarm in the industrial context is much more cumbersome as it might trigger harmeful action such as stopping the production or triggering expensive human interventions.  \n:::\n\nMoreover, the risk of false alarm is increased by the fact that in order to detect anomaly rapidly, one needs to make a classification over short windows inducing a number of decisions per day that are quite important. As an example, imagine a decision that is based on the examination of a sliding window that lasts 10 seconds. This induces 360 decisions per hour and 2880 decision in a working day of 8 hours. \n\nThis means that with a **false alarm rate of 1%**, whcih is considered to be quite small in the everyday applications mentioned above, one should expect more than **28 false alarms per day** which is obviously inacceptable and might lead to the alarm system being totally deactivated by the operators. \n\n## Sparsity and physics \n\nknowledge based physical laws are known to be a major source of sparsity. As an example, consider the most famous physical law one can imagine, namely, the Newton law in its translational form: \n\n$$\nm\\underbrace{\\dfrac{d^2r}{dt^2}}_{a_{cc}} = F - \\nu\\underbrace{\\dfrac{dr}{dt}}_{v}\n$${#eq-newton}\n\nThis relation involves two coefficients which are, the mass $m$ and the friction coefficients $\\nu$ while three sensors are involved, namely: \n\n- The acceleration $a_{cc}$ ($m.s^{-2}$)\n\n- The speed $v$ ($m.s^{-1}$)\n\n- The traction force ($N$).\n\nThe important fact here is the following: \n\n:::{.callout-important title=\"Context-independent law\"}\nWhatever is the context of use (high speed, high acceleration, deceleration, steady speed, random traction time profile), the relationship expressed by @eq-newton holds always true and only depends on the pair of scalars $m$ and $\\nu$. \n\nIn other word, this relationship is **context-independent**. It represents a view of the normality of the system. \n:::\n\nNow assume that the traction force is not measured and it is rather the angular position of the accelerator pedal that is measured, say $\\theta$. In this case, the raltionship, in terms of the new set of sensors, takes the form: \n\n$$\nm\\underbrace{\\dfrac{d^2r}{dt^2}}_{a_{cc}} = \\underbrace{(k_1\\theta+k_2\\theta^2)}_{F} - \\nu\\underbrace{\\dfrac{dr}{dt}}_{v}\n$${#eq-newton2}\n\nwhere the quadratic expression $(k_1\\theta+k_2\\theta^2)$ is suppsed to represent the traction force[^exper].\n\nWith this new set of measurement, it is @eq-newton2 that represents the normality of the system in a way that is still context-independent.\n\nFrom the above simple and intuitive example, it comes out that: \n\n:::{.callout-note title=\"physical laws are sparse\"}\nAlmost any context of use might be sufficient to capture the values of the parameters $(m, k_1, k_2, \\nu)$ leading to a relationship that holds over **all other unseen conttexts**. Therefore no alarm would be raised when these unseen contexts arise because the residual remains small. \n:::\n\nThe `mizopol` package's philosophy lies in the search for **parsimonious** relationships that are polynomial or piece wise-polynomial. This is inspired by the discussion above. \n\nIn the following section, a simple illustrative example is given to show the superiority of sparse solution in addressing the normality characteriation outside the space spanned by the samples present in the training dataset. \n\nThe [following section](parsimony_oscillator.qmd) shows another example of physical system highlighting the difference between a **blind normality characterization** and the one based on parsimonious **physically-inspired normality characterization**.\n\n[^exper]: Assuming that the pair $(k_1,k_2)$ is *identified* via some factory data-acquisition experiments. \n","srcMarkdownNoYaml":"\n\n***\n\n## Introduction \n\nIn this section, we discuss the importance of **parsimonious models** when addressing the characterization of normality in industrial data, particularly in the framework of normality characterization and anomaly detection. \n\nFirst of all, an informal discussion is proposed before a detailed use-case is shown that explicitly illustrates the relevance of deriving parsimonious models. \n\n:::{.callout-note title=\"Parsimony vs sparsity\"}\nThe words *parsimony* and *sparsity* are intimately linked and this is why sometimes, they are interchageably used. Roughly speaking, a model is described as parsimonious if it is defined through a vector of parameters that is sparse (contains many zeros). \n\nConsequently, one can state that **looking for sparsity**, at the search step,  induces parsimonious models. \n:::\n\n\n## Why sparsity is relevant in industry?\n\n### Anomaly detection \n\nThe parsimony, by helping avoiding *hasardous excursions* when the features go away from the domain spanned by the training data, improves the quality of the detection as it is suggested in the following sketchy representation:\n\n:::{.panel-tabset}\n\n## Parsimonious \n\n![Since the model (in Green) is parsimonious and still fits almost all points, the red one can be declared to be anomalous **with high confidence**.](images/parsimony1){width=80%}\n\n## Non parsimonious\n\n![Because the model (in Green) is not parsimonuous, the probability it overfit the data is greater making the diagnosis of the red circle more difficult to assess with confidence.](images/parsimony2){width=80%}\n:::\n\nThis example shows that: \n\n:::{.callout-tip title=\"Sparsity induces better detectability\"}\nsparsity helps detecting true anomalies compared to *nervous* models that might wrongly visit irrelevant regions inducing false normality bias. \n:::\n\nWhile this simple example highlights the relevance of sparsity in detecting true anomalies, the following discussion focuses on its role in avoinding false alarm upon changes in the context of use. \n\n\nThe need for sparsity in the industrial case stems from two major specificities, namely: The curse of contexts and the aversion to false alarms. This issues are explained hereafter.\n\n### The curse of context\n\nOne of the major obstacles to a correct normality characterization is linked to the fact that: \n\nThe training data scarcely contains all possible contexts\n: Take a robot for instance, the context of use might be represented by the types of trajectories, the types of tasks, the excursion of the kinematic or dynamic variables contained. On all these items, the training data might not represent all the different contexts. \n\n:::{.callout-warning title=\"False alarm on new contexts\"}\nThe multiplicity of contexts of use and their highly probable absence in the training data enhances the risk of raising **false alarm** because of the resurgence of an unseen contexts rather than because of a real change in the system's integrity or health parameters. \n\nIn the [Anomalies vs operational changes](state_context_anomalies.qmd) section, a more detailed visual explanation of the state and context-induced anomalies that need to be distinguished from parameter changes (induced by faults) in dynamical systems. \n:::\n\n### The aversion to false alarms\n\nAnother specificity of industrial applications is that:\n\nIndustry is very sensitive to false alarms\n: Compared to a bad classification in images or an erroneous book recommendation:\n\n:::{.callout-tip title='The cost of False alarm in industry'}\n a false alarm in the industrial context is much more cumbersome as it might trigger harmeful action such as stopping the production or triggering expensive human interventions.  \n:::\n\nMoreover, the risk of false alarm is increased by the fact that in order to detect anomaly rapidly, one needs to make a classification over short windows inducing a number of decisions per day that are quite important. As an example, imagine a decision that is based on the examination of a sliding window that lasts 10 seconds. This induces 360 decisions per hour and 2880 decision in a working day of 8 hours. \n\nThis means that with a **false alarm rate of 1%**, whcih is considered to be quite small in the everyday applications mentioned above, one should expect more than **28 false alarms per day** which is obviously inacceptable and might lead to the alarm system being totally deactivated by the operators. \n\n## Sparsity and physics \n\nknowledge based physical laws are known to be a major source of sparsity. As an example, consider the most famous physical law one can imagine, namely, the Newton law in its translational form: \n\n$$\nm\\underbrace{\\dfrac{d^2r}{dt^2}}_{a_{cc}} = F - \\nu\\underbrace{\\dfrac{dr}{dt}}_{v}\n$${#eq-newton}\n\nThis relation involves two coefficients which are, the mass $m$ and the friction coefficients $\\nu$ while three sensors are involved, namely: \n\n- The acceleration $a_{cc}$ ($m.s^{-2}$)\n\n- The speed $v$ ($m.s^{-1}$)\n\n- The traction force ($N$).\n\nThe important fact here is the following: \n\n:::{.callout-important title=\"Context-independent law\"}\nWhatever is the context of use (high speed, high acceleration, deceleration, steady speed, random traction time profile), the relationship expressed by @eq-newton holds always true and only depends on the pair of scalars $m$ and $\\nu$. \n\nIn other word, this relationship is **context-independent**. It represents a view of the normality of the system. \n:::\n\nNow assume that the traction force is not measured and it is rather the angular position of the accelerator pedal that is measured, say $\\theta$. In this case, the raltionship, in terms of the new set of sensors, takes the form: \n\n$$\nm\\underbrace{\\dfrac{d^2r}{dt^2}}_{a_{cc}} = \\underbrace{(k_1\\theta+k_2\\theta^2)}_{F} - \\nu\\underbrace{\\dfrac{dr}{dt}}_{v}\n$${#eq-newton2}\n\nwhere the quadratic expression $(k_1\\theta+k_2\\theta^2)$ is suppsed to represent the traction force[^exper].\n\nWith this new set of measurement, it is @eq-newton2 that represents the normality of the system in a way that is still context-independent.\n\nFrom the above simple and intuitive example, it comes out that: \n\n:::{.callout-note title=\"physical laws are sparse\"}\nAlmost any context of use might be sufficient to capture the values of the parameters $(m, k_1, k_2, \\nu)$ leading to a relationship that holds over **all other unseen conttexts**. Therefore no alarm would be raised when these unseen contexts arise because the residual remains small. \n:::\n\nThe `mizopol` package's philosophy lies in the search for **parsimonious** relationships that are polynomial or piece wise-polynomial. This is inspired by the discussion above. \n\nIn the following section, a simple illustrative example is given to show the superiority of sparse solution in addressing the normality characteriation outside the space spanned by the samples present in the training dataset. \n\nThe [following section](parsimony_oscillator.qmd) shows another example of physical system highlighting the difference between a **blind normality characterization** and the one based on parsimonious **physically-inspired normality characterization**.\n\n[^exper]: Assuming that the pair $(k_1,k_2)$ is *identified* via some factory data-acquisition experiments. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":"mathjax","css":["styles/styles.css"],"toc":true,"output-file":"parsimony.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","bibliography":["biblio.bib"],"resources":["images/sparsity_osc_*.png","images/anomalies_detection_pbstat_*.png","images/sources_of_anomalies_*.png"],"theme":"flatly","link-citations":true,"csl":"styles/apa.csl","title":"Parsimony & sparse models","subtitle":"Why parsimony is crucial in industrial data-driven solutions?"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}