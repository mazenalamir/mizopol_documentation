{"title":"Piece-wise polynomial invariants","markdown":{"yaml":{"title":"Piece-wise polynomial invariants","subtitle":"Using pwpol to design invariants for robot manipulators"},"headingText":"The dataset","containsRefs":false,"markdown":"\n\n*** \n\n\n:::{.columns}\n\n:::{.column width=60%}\nWe dispose of a dataset containing the recordings of the angular positions, velocities and accelerations of the four links as well as the associated applied torques[^current]. \n\nFrom the measurements dataframe shown below, it can be seen that there are 16 columns (12 of which are used as features) which are those linked to the kinematic variables (three for each link). The remaining four columns stand for the torque at the four links. \n\n_The dataset contains 1,617,936 rows_.\n:::\n\n:::{.column width=40%}\n![](images/staubliPhoto.png){fig-align=\"right\" width=45%}\n:::\n:::\n\n![The dataframe used in this section.](images/dfrobot.png){fig-align=\"left\"}\n\n\n## Train/Test split\n\nThe following script splits[^withoutshuffle] the dataset into train and test subsets:\n\n```python\ntest_size = 0.95\nnTrain = int((1-test_size)*len(df_robot))\ndfTrain = df_robot.iloc[0:nTrain]\ndfTest = df_robot.iloc[nTrain:]\n```\n\n## Fitting an invariant using `pwpol`\n\nRecall that the objective of the `pwpol` module is to find a set of multivariate polynomials (here polynomials of $12$ variables), say $P_i$, $i=1,\\dots,n_r$ such that one gets a small residual of the following form: \n\n$$\ne = \\min_{i=1,\\dots,n_r}\\left\\vert y-P_i(x)\\right\\vert\n$$\n\nover the samples contained in the training dataset.\n\nThe following script sets some of the search parameters (leaving the remaining ones to their default values) and calls the fitting function `fit_pwp_models` accordingly: \n\n```python \nfrom pwpol import fit_pwp_models, plot, DIC_ARGS\nfrom time import time\n\n# Choose which columns to use as features \ncolX = [c for c in dfTrain.columns if 'torque' not in c]\n\n# and which is to be used as label y\ncoly = 'torque1'\n\n# Download the default values dictionary \ndic_args = DIC_ARGS\n\n# and change some of its entries:\ndic_args.update(dict(\n    df=dfTrain, \n    colX=colX, \n    coly=coly,\n    th=0.2, \n    deg=1, \n    window=200\n))\n\nt1 = time()\nmodel, _ = fit_pwp_models(**dic_args)\nprint(f'cpu={time()-t1:3.2f} sec')\n```\n\n:::{.callout-note title='Parameters setting'}\nNotice that the script above attempts to fit a piece-linear relationship (deg=1) with a threshold 0.2 in the sense of the following acceptability criterion to adopt a polynomial in the set of useful ones: \n\n$$\n\\dfrac{\\texttt{percentile}(y-\\hat y, 95)}{\\texttt{median}(\\vert y\\vert )}\\le \\texttt{th}\n$$\nNotice however that the value of the threshold $\\texttt{th}$ is **first set** to the corresponding argument of the `dic_args` used in the call of the `fit_pwp_models` but **it might be increased** after a given number of failures, this explains the following log content where the final percentile needed a higher value of the threshold.\n:::\n\nThe previous script produces the following output: \n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.200 \nTreated  36% | #rows=  51856 | #models =   1 | #coeffs =   6, | th=0.200 \nTreated  54% | #rows=  37623 | #models =   2 | #coeffs =  18, | th=0.200 \nTreated  69% | #rows=  25240 | #models =   3 | #coeffs =  29, | th=0.200 \nTreated  80% | #rows=  16748 | #models =   4 | #coeffs =  37, | th=0.200 \nTreated  90% | #rows=   8495 | #models =   5 | #coeffs =  45, | th=0.200 \nTreated  95% | #rows=   4651 | #models =   6 | #coeffs =  51, | th=0.200 \nTreated  97% | #rows=   2798 | #models =   7 | #coeffs =  59, | th=0.200 \nTreated  99% | #rows=    845 | #models =   8 | #coeffs =  66, | th=0.200 \nTreated 100% | #rows=     14 | #models =   9 | #coeffs =  77, | th=1.032 \n\ncpu=101.75 sec\n```\n:::\n\nThis log shows provides, among others, the following facts:\n\n- The solution has been found in less than two minutes\n- The solution involves $n_r=9$ models \n- The total number of monomials (non zero coefficients) is equal to 77. \n\n## Validation of the solution \n\nIn order to validate the fitted solution, we need to use the piece-wise relationship in order to predict the residual on the test sub-dataset. \n\nSince the training dataset is only 5% of the whole data, we simply predict the residual on the whole dataset while showing the training region by a shaded area. \n\nThis is done in the following script:\n\n```python \nfrom pwpol import predict \n\n# set the true value \nytrue = df_robot[coly]\n\n# predict the closest output of the polynomials\nYpred, ypred, per_e = predict(df_robot, model, options=dict(y=ytrue))\n\n# plot the result\nfig = go.Figure()\nt = np.arange(0, len(ytrue))\nfig.add_trace(go.Scatter(x=t, y=ytrue, name='True'))\nfig.add_trace(go.Scatter(x=t, y=ypred, name='predicted'))\n\n# ... some other plotting instructions for the shaded region\n# and the title ... \n\nfig.show()\n```\n\n:::{.callout-tip title='Result for th=0.2'}\n<iframe src=\"html_files/staubli_res_1.html\" width=\"130%\" height=550\" style=\"border:20px;\"></iframe>\n:::\n\nPerforming *zoom operations* on the figure above enable to better appreciate how small the residual is with only $9$ linear multivariate polynomials involving 77 coefficients. \n\n## Precision vs sparsity \n\nNow what if we increase the precision threshold from $\\texttt{th}=0.2$ to $\\texttt{th}=0.3$. This would obviously leads to less precise matching but might reduce the complexity of the invariance relationship. \n\nThis indeed materializes in the output of the process that is reported hereafter: \n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.300 \nTreated  53% | #rows=  38086 | #models =   1 | #coeffs =   7, | th=0.300 \nTreated  73% | #rows=  22556 | #models =   2 | #coeffs =  14, | th=0.300 \nTreated  88% | #rows=  10178 | #models =   3 | #coeffs =  20, | th=0.300 \nTreated  95% | #rows=   4389 | #models =   4 | #coeffs =  28, | th=0.300 \nTreated  99% | #rows=   1584 | #models =   5 | #coeffs =  36, | th=0.300 \nTreated 100% | #rows=    463 | #models =   6 | #coeffs =  46, | th=0.300 \n\ncpu=62.25558400154114\n```\n:::\n\nNow we get a relationship involving only $n_r=6$ polynomials instead of $9$ before and only $46$ monomials instead of $77$ in the previous setting. \n\nObviously, by examining the plot below, it is possible to see that the precision is slightly impacted although it remains quite decent. The appropriate tuning is obviously problem-dependent.\n\n:::{.callout-tip title='Result for th=0.3'}\n<iframe src=\"html_files/staubli_res_2.html\" width=\"130%\" height=550\" style=\"border:20px;\"></iframe>\n:::\n\nThe inverse process can be also attempted by reducing the threshold to $\\texttt{th}=0.1$ seeking a more precise residual which leads to the following results:\n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.100 \nTreated  19% | #rows=  66267 | #models =   1 | #coeffs =   8, | th=0.100 \nTreated  32% | #rows=  55029 | #models =   2 | #coeffs =  14, | th=0.100 \nTreated  43% | #rows=  46530 | #models =   3 | #coeffs =  22, | th=0.100 \nTreated  57% | #rows=  35048 | #models =   4 | #coeffs =  31, | th=0.100 \nTreated  68% | #rows=  26354 | #models =   5 | #coeffs =  41, | th=0.100 \nTreated  79% | #rows=  17625 | #models =   6 | #coeffs =  50, | th=0.100 \nTreated  81% | #rows=  15649 | #models =   7 | #coeffs =  58, | th=0.100 \nTreated  85% | #rows=  12697 | #models =   8 | #coeffs =  71, | th=0.100 \nTreated  87% | #rows=  10701 | #models =   9 | #coeffs =  81, | th=0.100 \nTreated  89% | #rows=   9045 | #models =  10 | #coeffs =  89, | th=0.100 \nTreated  90% | #rows=   8128 | #models =  11 | #coeffs = 100, | th=0.100 \nTreated  92% | #rows=   7156 | #models =  12 | #coeffs = 110, | th=0.100 \nTreated  93% | #rows=   6143 | #models =  13 | #coeffs = 118, | th=0.120 \nTreated  94% | #rows=   5284 | #models =  14 | #coeffs = 129, | th=0.144 \nTreated  95% | #rows=   4273 | #models =  15 | #coeffs = 139, | th=0.173 \nTreated  97% | #rows=   3115 | #models =  16 | #coeffs = 149, | th=0.249 \nTreated  98% | #rows=   2162 | #models =  17 | #coeffs = 158, | th=0.249 \nTreated  99% | #rows=   1068 | #models =  18 | #coeffs = 167, | th=0.430 \n\ncpu=101.70363187789917\n```\n:::\n\nAn easier comparison between the three setting can be obtained by looking at the percentile of normalized error (ratio to the median of the absolute value of the target) for the different values $\\texttt{th}=0.1, 0.2, 0.3$. The same results are also shown for an indentification of a single $3$rd order polynomial via the `plars` module introduced in the [dedicated section](plars_intro.qmd):\n\n:::{.columns}\n\n:::{.column width=60%}\n![The percentiles of normalized residuals for the three piece-wise represenation of invariant relationship corresponding to different settings of the initial admissible precision threshold used in `pwpol`.](images/df_diffth_robot.png){width=65% fig-align=\"center\"}\n:::\n\n:::{.column width=40%}\n![The percentiles of normalized residuals for the single third order polynomial identified using the `plars` module.](images/robot_plars_percentile.png){width=100% fig-align=\"center\"}\n:::\n\n:::\n\nThe comparison inside the piece-wise polynomial solutions shows the relevance of the precition threshold $\\texttt{th}$ inside the `pwpol` module. \n\nOn the other hand, the comparison between these solutions and the single polynomial solution **clearly highlights the high relevance of the piece-wise polynomial structure** in capturing the tighter relationship keeping in mind that one solution is implicit and hence serves only for residual definition while the other is explicit and hence induce prediction power. \n\n:::{.callout-note title='Smaller residuals with lesser complexity'}\nNotice that the residuals for the solution with $\\texttt{th}=0.2, 0.3$, respectively obtained using **77** and **46** coefficients are much smaller than the single polynomial associated residual depsite the fact that the latter is obtained using **125** coefficients. \n:::\n\n## Further readings \n\nA deep comparison has been recently proposed in (@ALAMIRstaubli2025) between the piece-wise multi-variate polynomial approach offered by the `pwpol` module and the well known GRU-DNN model. The comparison is done in terms of:\n\n- The residual precision \n- The complexity of the model \n- The computation time \n\n:::{.callout-note title='Comparison to GRU-DNN'}\nThe comparison shows that \n\n1) smaller residuals can be obtained \n\n2) with drastically lower number of active parameters (few hundred compared to several hundred of thousands for DNN)\n\n3) and a computation time that never exceeds two minutes against several hours for the DNN version \n\n:::\n\nMoreover, it has been shown using some synthetic defaults (alsmost difficult to see in the raw data) that the residual generator is able not only to detect the anomaly/default but also to tell which axis it seems to be originated from. \n\nThis is shown in the following figures[^seeformore]: \n\n::: {.panel-tabset}\n\n### Anomalies introduced on **axis 1**\n![Examples of anomalies detection using the piece-wise polynmial invariants when three different kinds of anomalies are introduced on axis 1. (Excerpt from @ALAMIRstaubli2025)](images/robot_axe_1_detection.png){width=100% fig-align=\"left\"}\n\n### Anomalies introduced on **axis 2**\n\n![Example of anomalies detection using the piece-wise polynmial invariants when three different kinds of anomalies are introduced on axis 2. (Excerpt from @ALAMIRstaubli2025)](images/robot_axe_2_detection.png){width=95% fig-align=\"left\"}\n\n:::\n\n\n:::{.callout-important title=\"Keep in mind!\"}\nIt is important to keep in mind that contrary to the **implicit** piece-wise relationships provided by the `pwpol` module, the GRU-DNN version provides an **explicit** predictor of the torque which is much more difficult to achieve. \n\nThis unerlines the difference between the two solutions and enables to understand why the results provided by `pwpol` show smaller residuals. \n\nNotice however that as far as the anomaly detection is targeted, the implicit/explicit nature of the relationship does not really matter which makes the comparison relevant depsite of the difference in nature between the two classes of models. \n::: \n\n[^current]: as measured from the associated currents. \n[^withoutshuffle]: by taking the first 5% to train and the remaining 95% to test\n[^seeformore]: See the reference mentioned above for more details. ","srcMarkdownNoYaml":"\n\n*** \n\n## The dataset \n\n:::{.columns}\n\n:::{.column width=60%}\nWe dispose of a dataset containing the recordings of the angular positions, velocities and accelerations of the four links as well as the associated applied torques[^current]. \n\nFrom the measurements dataframe shown below, it can be seen that there are 16 columns (12 of which are used as features) which are those linked to the kinematic variables (three for each link). The remaining four columns stand for the torque at the four links. \n\n_The dataset contains 1,617,936 rows_.\n:::\n\n:::{.column width=40%}\n![](images/staubliPhoto.png){fig-align=\"right\" width=45%}\n:::\n:::\n\n![The dataframe used in this section.](images/dfrobot.png){fig-align=\"left\"}\n\n\n## Train/Test split\n\nThe following script splits[^withoutshuffle] the dataset into train and test subsets:\n\n```python\ntest_size = 0.95\nnTrain = int((1-test_size)*len(df_robot))\ndfTrain = df_robot.iloc[0:nTrain]\ndfTest = df_robot.iloc[nTrain:]\n```\n\n## Fitting an invariant using `pwpol`\n\nRecall that the objective of the `pwpol` module is to find a set of multivariate polynomials (here polynomials of $12$ variables), say $P_i$, $i=1,\\dots,n_r$ such that one gets a small residual of the following form: \n\n$$\ne = \\min_{i=1,\\dots,n_r}\\left\\vert y-P_i(x)\\right\\vert\n$$\n\nover the samples contained in the training dataset.\n\nThe following script sets some of the search parameters (leaving the remaining ones to their default values) and calls the fitting function `fit_pwp_models` accordingly: \n\n```python \nfrom pwpol import fit_pwp_models, plot, DIC_ARGS\nfrom time import time\n\n# Choose which columns to use as features \ncolX = [c for c in dfTrain.columns if 'torque' not in c]\n\n# and which is to be used as label y\ncoly = 'torque1'\n\n# Download the default values dictionary \ndic_args = DIC_ARGS\n\n# and change some of its entries:\ndic_args.update(dict(\n    df=dfTrain, \n    colX=colX, \n    coly=coly,\n    th=0.2, \n    deg=1, \n    window=200\n))\n\nt1 = time()\nmodel, _ = fit_pwp_models(**dic_args)\nprint(f'cpu={time()-t1:3.2f} sec')\n```\n\n:::{.callout-note title='Parameters setting'}\nNotice that the script above attempts to fit a piece-linear relationship (deg=1) with a threshold 0.2 in the sense of the following acceptability criterion to adopt a polynomial in the set of useful ones: \n\n$$\n\\dfrac{\\texttt{percentile}(y-\\hat y, 95)}{\\texttt{median}(\\vert y\\vert )}\\le \\texttt{th}\n$$\nNotice however that the value of the threshold $\\texttt{th}$ is **first set** to the corresponding argument of the `dic_args` used in the call of the `fit_pwp_models` but **it might be increased** after a given number of failures, this explains the following log content where the final percentile needed a higher value of the threshold.\n:::\n\nThe previous script produces the following output: \n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.200 \nTreated  36% | #rows=  51856 | #models =   1 | #coeffs =   6, | th=0.200 \nTreated  54% | #rows=  37623 | #models =   2 | #coeffs =  18, | th=0.200 \nTreated  69% | #rows=  25240 | #models =   3 | #coeffs =  29, | th=0.200 \nTreated  80% | #rows=  16748 | #models =   4 | #coeffs =  37, | th=0.200 \nTreated  90% | #rows=   8495 | #models =   5 | #coeffs =  45, | th=0.200 \nTreated  95% | #rows=   4651 | #models =   6 | #coeffs =  51, | th=0.200 \nTreated  97% | #rows=   2798 | #models =   7 | #coeffs =  59, | th=0.200 \nTreated  99% | #rows=    845 | #models =   8 | #coeffs =  66, | th=0.200 \nTreated 100% | #rows=     14 | #models =   9 | #coeffs =  77, | th=1.032 \n\ncpu=101.75 sec\n```\n:::\n\nThis log shows provides, among others, the following facts:\n\n- The solution has been found in less than two minutes\n- The solution involves $n_r=9$ models \n- The total number of monomials (non zero coefficients) is equal to 77. \n\n## Validation of the solution \n\nIn order to validate the fitted solution, we need to use the piece-wise relationship in order to predict the residual on the test sub-dataset. \n\nSince the training dataset is only 5% of the whole data, we simply predict the residual on the whole dataset while showing the training region by a shaded area. \n\nThis is done in the following script:\n\n```python \nfrom pwpol import predict \n\n# set the true value \nytrue = df_robot[coly]\n\n# predict the closest output of the polynomials\nYpred, ypred, per_e = predict(df_robot, model, options=dict(y=ytrue))\n\n# plot the result\nfig = go.Figure()\nt = np.arange(0, len(ytrue))\nfig.add_trace(go.Scatter(x=t, y=ytrue, name='True'))\nfig.add_trace(go.Scatter(x=t, y=ypred, name='predicted'))\n\n# ... some other plotting instructions for the shaded region\n# and the title ... \n\nfig.show()\n```\n\n:::{.callout-tip title='Result for th=0.2'}\n<iframe src=\"html_files/staubli_res_1.html\" width=\"130%\" height=550\" style=\"border:20px;\"></iframe>\n:::\n\nPerforming *zoom operations* on the figure above enable to better appreciate how small the residual is with only $9$ linear multivariate polynomials involving 77 coefficients. \n\n## Precision vs sparsity \n\nNow what if we increase the precision threshold from $\\texttt{th}=0.2$ to $\\texttt{th}=0.3$. This would obviously leads to less precise matching but might reduce the complexity of the invariance relationship. \n\nThis indeed materializes in the output of the process that is reported hereafter: \n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.300 \nTreated  53% | #rows=  38086 | #models =   1 | #coeffs =   7, | th=0.300 \nTreated  73% | #rows=  22556 | #models =   2 | #coeffs =  14, | th=0.300 \nTreated  88% | #rows=  10178 | #models =   3 | #coeffs =  20, | th=0.300 \nTreated  95% | #rows=   4389 | #models =   4 | #coeffs =  28, | th=0.300 \nTreated  99% | #rows=   1584 | #models =   5 | #coeffs =  36, | th=0.300 \nTreated 100% | #rows=    463 | #models =   6 | #coeffs =  46, | th=0.300 \n\ncpu=62.25558400154114\n```\n:::\n\nNow we get a relationship involving only $n_r=6$ polynomials instead of $9$ before and only $46$ monomials instead of $77$ in the previous setting. \n\nObviously, by examining the plot below, it is possible to see that the precision is slightly impacted although it remains quite decent. The appropriate tuning is obviously problem-dependent.\n\n:::{.callout-tip title='Result for th=0.3'}\n<iframe src=\"html_files/staubli_res_2.html\" width=\"130%\" height=550\" style=\"border:20px;\"></iframe>\n:::\n\nThe inverse process can be also attempted by reducing the threshold to $\\texttt{th}=0.1$ seeking a more precise residual which leads to the following results:\n\n:::{.small}\n```verbatim\nTreated   0% | #rows=  80896 | #models =   0 | #coeffs =   0, | th=0.100 \nTreated  19% | #rows=  66267 | #models =   1 | #coeffs =   8, | th=0.100 \nTreated  32% | #rows=  55029 | #models =   2 | #coeffs =  14, | th=0.100 \nTreated  43% | #rows=  46530 | #models =   3 | #coeffs =  22, | th=0.100 \nTreated  57% | #rows=  35048 | #models =   4 | #coeffs =  31, | th=0.100 \nTreated  68% | #rows=  26354 | #models =   5 | #coeffs =  41, | th=0.100 \nTreated  79% | #rows=  17625 | #models =   6 | #coeffs =  50, | th=0.100 \nTreated  81% | #rows=  15649 | #models =   7 | #coeffs =  58, | th=0.100 \nTreated  85% | #rows=  12697 | #models =   8 | #coeffs =  71, | th=0.100 \nTreated  87% | #rows=  10701 | #models =   9 | #coeffs =  81, | th=0.100 \nTreated  89% | #rows=   9045 | #models =  10 | #coeffs =  89, | th=0.100 \nTreated  90% | #rows=   8128 | #models =  11 | #coeffs = 100, | th=0.100 \nTreated  92% | #rows=   7156 | #models =  12 | #coeffs = 110, | th=0.100 \nTreated  93% | #rows=   6143 | #models =  13 | #coeffs = 118, | th=0.120 \nTreated  94% | #rows=   5284 | #models =  14 | #coeffs = 129, | th=0.144 \nTreated  95% | #rows=   4273 | #models =  15 | #coeffs = 139, | th=0.173 \nTreated  97% | #rows=   3115 | #models =  16 | #coeffs = 149, | th=0.249 \nTreated  98% | #rows=   2162 | #models =  17 | #coeffs = 158, | th=0.249 \nTreated  99% | #rows=   1068 | #models =  18 | #coeffs = 167, | th=0.430 \n\ncpu=101.70363187789917\n```\n:::\n\nAn easier comparison between the three setting can be obtained by looking at the percentile of normalized error (ratio to the median of the absolute value of the target) for the different values $\\texttt{th}=0.1, 0.2, 0.3$. The same results are also shown for an indentification of a single $3$rd order polynomial via the `plars` module introduced in the [dedicated section](plars_intro.qmd):\n\n:::{.columns}\n\n:::{.column width=60%}\n![The percentiles of normalized residuals for the three piece-wise represenation of invariant relationship corresponding to different settings of the initial admissible precision threshold used in `pwpol`.](images/df_diffth_robot.png){width=65% fig-align=\"center\"}\n:::\n\n:::{.column width=40%}\n![The percentiles of normalized residuals for the single third order polynomial identified using the `plars` module.](images/robot_plars_percentile.png){width=100% fig-align=\"center\"}\n:::\n\n:::\n\nThe comparison inside the piece-wise polynomial solutions shows the relevance of the precition threshold $\\texttt{th}$ inside the `pwpol` module. \n\nOn the other hand, the comparison between these solutions and the single polynomial solution **clearly highlights the high relevance of the piece-wise polynomial structure** in capturing the tighter relationship keeping in mind that one solution is implicit and hence serves only for residual definition while the other is explicit and hence induce prediction power. \n\n:::{.callout-note title='Smaller residuals with lesser complexity'}\nNotice that the residuals for the solution with $\\texttt{th}=0.2, 0.3$, respectively obtained using **77** and **46** coefficients are much smaller than the single polynomial associated residual depsite the fact that the latter is obtained using **125** coefficients. \n:::\n\n## Further readings \n\nA deep comparison has been recently proposed in (@ALAMIRstaubli2025) between the piece-wise multi-variate polynomial approach offered by the `pwpol` module and the well known GRU-DNN model. The comparison is done in terms of:\n\n- The residual precision \n- The complexity of the model \n- The computation time \n\n:::{.callout-note title='Comparison to GRU-DNN'}\nThe comparison shows that \n\n1) smaller residuals can be obtained \n\n2) with drastically lower number of active parameters (few hundred compared to several hundred of thousands for DNN)\n\n3) and a computation time that never exceeds two minutes against several hours for the DNN version \n\n:::\n\nMoreover, it has been shown using some synthetic defaults (alsmost difficult to see in the raw data) that the residual generator is able not only to detect the anomaly/default but also to tell which axis it seems to be originated from. \n\nThis is shown in the following figures[^seeformore]: \n\n::: {.panel-tabset}\n\n### Anomalies introduced on **axis 1**\n![Examples of anomalies detection using the piece-wise polynmial invariants when three different kinds of anomalies are introduced on axis 1. (Excerpt from @ALAMIRstaubli2025)](images/robot_axe_1_detection.png){width=100% fig-align=\"left\"}\n\n### Anomalies introduced on **axis 2**\n\n![Example of anomalies detection using the piece-wise polynmial invariants when three different kinds of anomalies are introduced on axis 2. (Excerpt from @ALAMIRstaubli2025)](images/robot_axe_2_detection.png){width=95% fig-align=\"left\"}\n\n:::\n\n\n:::{.callout-important title=\"Keep in mind!\"}\nIt is important to keep in mind that contrary to the **implicit** piece-wise relationships provided by the `pwpol` module, the GRU-DNN version provides an **explicit** predictor of the torque which is much more difficult to achieve. \n\nThis unerlines the difference between the two solutions and enables to understand why the results provided by `pwpol` show smaller residuals. \n\nNotice however that as far as the anomaly detection is targeted, the implicit/explicit nature of the relationship does not really matter which makes the comparison relevant depsite of the difference in nature between the two classes of models. \n::: \n\n[^current]: as measured from the associated currents. \n[^withoutshuffle]: by taking the first 5% to train and the remaining 95% to test\n[^seeformore]: See the reference mentioned above for more details. "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":"mathjax","css":["styles/styles.css"],"toc":true,"output-file":"pwpol_robot.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","bibliography":["biblio.bib"],"resources":["images/sparsity_osc_*.png","images/anomalies_detection_pbstat_*.png","images/sources_of_anomalies_*.png"],"theme":"flatly","link-citations":true,"csl":"styles/apa.csl","title":"Piece-wise polynomial invariants","subtitle":"Using pwpol to design invariants for robot manipulators"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}