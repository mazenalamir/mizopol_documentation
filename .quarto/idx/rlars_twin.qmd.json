{"title":"rlars","markdown":{"yaml":{"title":"rlars","subtitle":"Illustrative example on the twin-pendulum system"},"headingText":"The system","containsRefs":false,"markdown":"\n\n***\n\n\n:::{.columns}\n\n:::{.column width=65%}\n\nIn this section an example of use of the `rlars` module is provided where it is used to characterize the normality via a reduced number of short experiments. \n\nThe system consists in two pendulums that share the same axis of rotation that is fixd on a moving cart. The only possible exogenous action is provided by the horizontal driving force applied to the cart. \n\nIt is assumed that the model is not known so that very sophisticated (and rationla) actions that would enable a rich exploration of the state space are not available. Only short and rapidly damped experiments are tolerated. \n\n{{< fa solid lightbulb >}} See this [funny video](https://www.mazenalamir.fr/home/images/videos/Double_pendule_1208.mp4) regarding the control of this system (should its model be known which is not the case in the present study). \n\n:::\n\n:::{.column width=2%}\n\n:::\n\n:::{.column width=33%}\n![The twin pendulum system for which `rlars` is used in order to characterize the normality from a reduced number of samples.](images/twinpendulum.png)\n:::\n\n:::\n\n***\n\n# Experiment design \n\nAn excitation generator function is designed that produces signals composed of `nModes` frequencies uniformly distributed on the interval $[0,\\texttt{omega\\_max}]$ and damped with an exponential decay rate of `mu`. \n\nThis is expressed in the following python function which returns a function of time representing the candidate excitation that is defined in a random way: \n\n```python\ndef create_udet(u_max, omega_max, nModes, mu):\n    '''\n    Facility that creates a function of of the time \n    that provides the control to be applied at the instant \n    used as argument. \n\n    u_max       : the maximum value of the control ampllitude \n    omega_max   : the maximum pulsation used in the excitation signal \n    nModes      : the number of modes from 0 to omega_max \n    mu          : The decrease rate of the signal.\n    '''\n    omegas = np.linspace(0, omega_max, nModes)\n    coeff = np.random.randn(nModes)    \n    angle = np.random.rand(nModes) * 2 * np.pi\n    M = lambda t: np.array([np.sin(omegas[i] * t +angle[i]) \n                            for i in range(nModes)])\n    def udet(t):\n        v = np.sum(M(t) * coeff) * np.exp(-mu * t) * (u_max/3)\n        return min(u_max, max(-u_max, v)) \n    \n    return udet\n```\n\nThe following figure shows three different resulting excitation profiles that mainly last around one minute each (you can zoom on the figure to closely examine the evolution in time): \n\n<div style=\"text-align: center;\">\n<iframe src=\"html_files/excitation_twin.html\" height=\"520\" width=\"100%\" style=\"border: 0.3px solid black;\"></iframe>\n</div>\n\n# Working dataset\n\nThe following steps have been used to built the working datasets:\n\n1. Four different experiments are *simulated* using four different random excitation profiles as defined in the previous section. \n\n2. In order to emulate the presence of mechanical stops limiting the linear movement of the cart, each simulation has been truncated from the instant where the stop is reached (the stops are positioned at $r_{max}=\\pm 100$). \n\n    Given that the sampling period for the measurement acquisition is set to $\\tau=0.01$ sec, the total available samples should the previous truncating step be avoided would have been equal to 20,000.  As a matter of fact the truncation leads to a **total dataset containing 11,822 samples**.\n\n3. Although the model is supposed to be unknown, it can be reasonably assumed that the underlying laws are defined in terms of the second derivatives:\n\n   $$\n   y_1 = \\ddot{\\theta}_1 \\quad,\\quad\n   y_2 = \\ddot{\\theta}_2 \\quad,\\quad\n   y_3 = \\ddot{r}\n   $$\n\n   being functions of the features vector:\n\n   $$\n   x = \\begin{bmatrix}\n   r & \\dot r & \\theta_1 & \\dot{\\theta}_1 & \\theta_2 &\n   \\dot\\theta_2 & u & \\sin\\theta_1 & \\cos\\theta_1 &\n   \\sin\\theta_2 & \\cos\\theta_2\n   \\end{bmatrix}\n   \\in \\mathbb{R}^{11}\n   $$\n\n   That is the reason why building the features matrix and the label vectors is completed using the python module `ml_derivatives`, which provides a robust reconstruction of the derivatives of noisy time series [see @ALAMIR2025112469 and the module description](https://mazenalamir.github.io/ml_derivatives/).\n\n   ::: {.callout-tip title=\"5% noise added before computing the high derivatives\"}\n   In order to emulate real-life noisy conditions, a relative white noise has been added with a standard deviation of 5% of the 95th percentile of each of the simulated profiles $\\theta_1$, $\\theta_2$, and $r$.  \n   \n   These noisy versions are used in the reconstruction of the derivatives and hence in the resulting working dataset.\n   :::\n   \n# Identification results | comparison to `plars` \n\nIn order to highlight the advantage of using the `rlars` module over the simple use of `plars`, we first show the identification results when using the latter. \n\nAs far as the normalized error is concerned, it is defined as a ratio of the $95%$-quantile of the residual to the median of the norm of the label, namely: \n\n$$\n\\texttt{normalized error}= \\dfrac{\\texttt{percentile}(\\texttt{residual}, 95)}{\\texttt{median}(\\vert y\\vert)}\n$$\n\n## `plars` (explicit polynomials)\n\nThe following script and dataframe showing the statistics of the normalized error when multi-variate polynomial's identification is attempted via the `plars` module. \n\n:::{.panel-tabset}\n\n### Python script | cardinality\n\n```python\nfrom plars import PLARS, predict\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\n\n\nwindow=1000\ndeg=3\nnModels = 40\nnModes = 20\neps = 0.02\ntest_size=0.1\n\npl = PLARS(window=window, deg=deg, nModels=nModels, nModes=nModes, eps=eps)\n\npanel_plars = []\n\ni = 1\nfor z in [y1, y2, y3]:\n\n    Xid = (X-X.mean(axis=0))/X.std(axis=0)\n    Xtrain, Xtest, ytrain, ytest = train_test_split(Xid, z, test_size=test_size, shuffle=False)\n\n    solution  = pl.fit(Xtrain, ytrain)\n    print(f\"plars solution's cardinality for y{i} = {solution['card']}\")\n    dfi = solution['dfe_train']\n    dfi.columns = [f'Error on y{i}']\n    i += 1\n    panel_plars.append(dfi)\n\ndfRes_plars = pd.concat(panel_plars, axis=1)\ndfRes_plars\n```\n\n```verbatim\nplars solution's cardinality for y1 = 251\nplars solution's cardinality for y2 = 256\nplars solution's cardinality for y3 = 237\n```\n\n### Normalized residuals\n\n![](images/dfRes_plars_twin.png){width=40% fig-align=\"center\"}\n\nThese results are those obtained on the train data. They are *sufficiently bad* to make the examination of the result on test data useless. \n\n:::\n\n## `rlars`: (labels as polynomials' roots)\n\nIn this attemps, we use the same parameters except the degree that is reduced to `deg`=2 in order to emphasize the fact that the underlying strucutre is sufficiently rich for a lower order model to capture the relationships drastically better that the polynomial even with lower degree. \n\n:::{.panel-tabset}\n\n### Python script | cardinality\n\n```python\nfrom rlars import RLARS, compute_c_pol\nfrom plars import normalized_error\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\n\n\nwindow=1000\ndeg=2\nnModels = 40\nnModes = 20\neps = 0.02\ntest_size=0.4\n\nrl = RLARS(window=window, deg=deg, nModels=nModels, nModes=nModes, eps=eps)\n\npanel_rlars_train = []\npanel_rlars_test = []\n\ni = 1\nfor z in [y1, y2, y3]:\n\n    # prepare the train and test data\n    Xid = (X-X.mean(axis=0))/X.std(axis=0)\n    Xtrain, Xtest, ytrain, ytest = train_test_split(Xid, z, test_size=test_size, shuffle=False)\n\n    # fit the rlars instance\n    solution  = rl.fit(Xtrain, ytrain)\n    print(f\"rlars solution's cardinality for y{i} = {solution['card']}\")\n    dfi = solution['dfe_train']\n    dfi.columns = [f'Residual on y{i}']\n\n    # compute the residual on test data\n    c_pol = compute_c_pol(Xtest, solution)\n    ypred = np.array([np.polyval(c_pol[i,:][::-1], ytest[i]) \n                      for i in range(len(ytest))])\n    dfi_test = normalized_error(ytest, ypred)\n    dfi_test.columns = [f'Residual on y{i}']\n    panel_rlars_test.append(dfi_test)\n    i += 1\n    panel_rlars_train.append(dfi)\n\ndfRes_rlars_train = pd.concat(panel_rlars_train, axis=1)\ndfRes_rlars_test = pd.concat(panel_rlars_test, axis=1)\n```\n\n```verbatim\nrlars solution's cardinality for y1 = 97\nrlars solution's cardinality for y2 = 206\nrlars solution's cardinality for y3 = 5\n```\n\n### Normalized residuals\n\n![](images/dfRes_rlars_twin.png){width=60% fig-align=\"left\"}\n\n:::\n\nIn order to be able to appreciate the generalization power of the model that has been identified using quite small amount of data. It is important de recall the two following facts: \n\n1. The train/test split has been performed **without shuffle** with $\\texttt{test\\_size}=0.4$ meaning that the first 60% of the scenario is used for the train while the remaining 40% is used to test.\n\n2. The following figures show the four different scenarios used: \n\n:::{.panel-tabset}\n\n### Experiment #1\n\n```{=html}\n{{< include html_files/twin_plot_1.html >}}\n```\n\n### Experiment #2\n\n```{=html}\n{{< include html_files/twin_plot_2.html >}}\n```\n\n### Experiment #3\n\n```{=html}\n{{< include html_files/twin_plot_3.html >}}\n```\n\n### Experiment #4\n\n```{=html}\n{{< include html_files/twin_plot_4.html >}}\n```\n:::\n\n:::{.callout-warning title='Train & test among experiments'}\nIt comes from the observation of the length of each of the four scenarios that the training data concerns almost only the first two experiments while the last two experiments are part of the test data. \n:::","srcMarkdownNoYaml":"\n\n***\n\n# The system\n\n:::{.columns}\n\n:::{.column width=65%}\n\nIn this section an example of use of the `rlars` module is provided where it is used to characterize the normality via a reduced number of short experiments. \n\nThe system consists in two pendulums that share the same axis of rotation that is fixd on a moving cart. The only possible exogenous action is provided by the horizontal driving force applied to the cart. \n\nIt is assumed that the model is not known so that very sophisticated (and rationla) actions that would enable a rich exploration of the state space are not available. Only short and rapidly damped experiments are tolerated. \n\n{{< fa solid lightbulb >}} See this [funny video](https://www.mazenalamir.fr/home/images/videos/Double_pendule_1208.mp4) regarding the control of this system (should its model be known which is not the case in the present study). \n\n:::\n\n:::{.column width=2%}\n\n:::\n\n:::{.column width=33%}\n![The twin pendulum system for which `rlars` is used in order to characterize the normality from a reduced number of samples.](images/twinpendulum.png)\n:::\n\n:::\n\n***\n\n# Experiment design \n\nAn excitation generator function is designed that produces signals composed of `nModes` frequencies uniformly distributed on the interval $[0,\\texttt{omega\\_max}]$ and damped with an exponential decay rate of `mu`. \n\nThis is expressed in the following python function which returns a function of time representing the candidate excitation that is defined in a random way: \n\n```python\ndef create_udet(u_max, omega_max, nModes, mu):\n    '''\n    Facility that creates a function of of the time \n    that provides the control to be applied at the instant \n    used as argument. \n\n    u_max       : the maximum value of the control ampllitude \n    omega_max   : the maximum pulsation used in the excitation signal \n    nModes      : the number of modes from 0 to omega_max \n    mu          : The decrease rate of the signal.\n    '''\n    omegas = np.linspace(0, omega_max, nModes)\n    coeff = np.random.randn(nModes)    \n    angle = np.random.rand(nModes) * 2 * np.pi\n    M = lambda t: np.array([np.sin(omegas[i] * t +angle[i]) \n                            for i in range(nModes)])\n    def udet(t):\n        v = np.sum(M(t) * coeff) * np.exp(-mu * t) * (u_max/3)\n        return min(u_max, max(-u_max, v)) \n    \n    return udet\n```\n\nThe following figure shows three different resulting excitation profiles that mainly last around one minute each (you can zoom on the figure to closely examine the evolution in time): \n\n<div style=\"text-align: center;\">\n<iframe src=\"html_files/excitation_twin.html\" height=\"520\" width=\"100%\" style=\"border: 0.3px solid black;\"></iframe>\n</div>\n\n# Working dataset\n\nThe following steps have been used to built the working datasets:\n\n1. Four different experiments are *simulated* using four different random excitation profiles as defined in the previous section. \n\n2. In order to emulate the presence of mechanical stops limiting the linear movement of the cart, each simulation has been truncated from the instant where the stop is reached (the stops are positioned at $r_{max}=\\pm 100$). \n\n    Given that the sampling period for the measurement acquisition is set to $\\tau=0.01$ sec, the total available samples should the previous truncating step be avoided would have been equal to 20,000.  As a matter of fact the truncation leads to a **total dataset containing 11,822 samples**.\n\n3. Although the model is supposed to be unknown, it can be reasonably assumed that the underlying laws are defined in terms of the second derivatives:\n\n   $$\n   y_1 = \\ddot{\\theta}_1 \\quad,\\quad\n   y_2 = \\ddot{\\theta}_2 \\quad,\\quad\n   y_3 = \\ddot{r}\n   $$\n\n   being functions of the features vector:\n\n   $$\n   x = \\begin{bmatrix}\n   r & \\dot r & \\theta_1 & \\dot{\\theta}_1 & \\theta_2 &\n   \\dot\\theta_2 & u & \\sin\\theta_1 & \\cos\\theta_1 &\n   \\sin\\theta_2 & \\cos\\theta_2\n   \\end{bmatrix}\n   \\in \\mathbb{R}^{11}\n   $$\n\n   That is the reason why building the features matrix and the label vectors is completed using the python module `ml_derivatives`, which provides a robust reconstruction of the derivatives of noisy time series [see @ALAMIR2025112469 and the module description](https://mazenalamir.github.io/ml_derivatives/).\n\n   ::: {.callout-tip title=\"5% noise added before computing the high derivatives\"}\n   In order to emulate real-life noisy conditions, a relative white noise has been added with a standard deviation of 5% of the 95th percentile of each of the simulated profiles $\\theta_1$, $\\theta_2$, and $r$.  \n   \n   These noisy versions are used in the reconstruction of the derivatives and hence in the resulting working dataset.\n   :::\n   \n# Identification results | comparison to `plars` \n\nIn order to highlight the advantage of using the `rlars` module over the simple use of `plars`, we first show the identification results when using the latter. \n\nAs far as the normalized error is concerned, it is defined as a ratio of the $95%$-quantile of the residual to the median of the norm of the label, namely: \n\n$$\n\\texttt{normalized error}= \\dfrac{\\texttt{percentile}(\\texttt{residual}, 95)}{\\texttt{median}(\\vert y\\vert)}\n$$\n\n## `plars` (explicit polynomials)\n\nThe following script and dataframe showing the statistics of the normalized error when multi-variate polynomial's identification is attempted via the `plars` module. \n\n:::{.panel-tabset}\n\n### Python script | cardinality\n\n```python\nfrom plars import PLARS, predict\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\n\n\nwindow=1000\ndeg=3\nnModels = 40\nnModes = 20\neps = 0.02\ntest_size=0.1\n\npl = PLARS(window=window, deg=deg, nModels=nModels, nModes=nModes, eps=eps)\n\npanel_plars = []\n\ni = 1\nfor z in [y1, y2, y3]:\n\n    Xid = (X-X.mean(axis=0))/X.std(axis=0)\n    Xtrain, Xtest, ytrain, ytest = train_test_split(Xid, z, test_size=test_size, shuffle=False)\n\n    solution  = pl.fit(Xtrain, ytrain)\n    print(f\"plars solution's cardinality for y{i} = {solution['card']}\")\n    dfi = solution['dfe_train']\n    dfi.columns = [f'Error on y{i}']\n    i += 1\n    panel_plars.append(dfi)\n\ndfRes_plars = pd.concat(panel_plars, axis=1)\ndfRes_plars\n```\n\n```verbatim\nplars solution's cardinality for y1 = 251\nplars solution's cardinality for y2 = 256\nplars solution's cardinality for y3 = 237\n```\n\n### Normalized residuals\n\n![](images/dfRes_plars_twin.png){width=40% fig-align=\"center\"}\n\nThese results are those obtained on the train data. They are *sufficiently bad* to make the examination of the result on test data useless. \n\n:::\n\n## `rlars`: (labels as polynomials' roots)\n\nIn this attemps, we use the same parameters except the degree that is reduced to `deg`=2 in order to emphasize the fact that the underlying strucutre is sufficiently rich for a lower order model to capture the relationships drastically better that the polynomial even with lower degree. \n\n:::{.panel-tabset}\n\n### Python script | cardinality\n\n```python\nfrom rlars import RLARS, compute_c_pol\nfrom plars import normalized_error\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\n\n\nwindow=1000\ndeg=2\nnModels = 40\nnModes = 20\neps = 0.02\ntest_size=0.4\n\nrl = RLARS(window=window, deg=deg, nModels=nModels, nModes=nModes, eps=eps)\n\npanel_rlars_train = []\npanel_rlars_test = []\n\ni = 1\nfor z in [y1, y2, y3]:\n\n    # prepare the train and test data\n    Xid = (X-X.mean(axis=0))/X.std(axis=0)\n    Xtrain, Xtest, ytrain, ytest = train_test_split(Xid, z, test_size=test_size, shuffle=False)\n\n    # fit the rlars instance\n    solution  = rl.fit(Xtrain, ytrain)\n    print(f\"rlars solution's cardinality for y{i} = {solution['card']}\")\n    dfi = solution['dfe_train']\n    dfi.columns = [f'Residual on y{i}']\n\n    # compute the residual on test data\n    c_pol = compute_c_pol(Xtest, solution)\n    ypred = np.array([np.polyval(c_pol[i,:][::-1], ytest[i]) \n                      for i in range(len(ytest))])\n    dfi_test = normalized_error(ytest, ypred)\n    dfi_test.columns = [f'Residual on y{i}']\n    panel_rlars_test.append(dfi_test)\n    i += 1\n    panel_rlars_train.append(dfi)\n\ndfRes_rlars_train = pd.concat(panel_rlars_train, axis=1)\ndfRes_rlars_test = pd.concat(panel_rlars_test, axis=1)\n```\n\n```verbatim\nrlars solution's cardinality for y1 = 97\nrlars solution's cardinality for y2 = 206\nrlars solution's cardinality for y3 = 5\n```\n\n### Normalized residuals\n\n![](images/dfRes_rlars_twin.png){width=60% fig-align=\"left\"}\n\n:::\n\nIn order to be able to appreciate the generalization power of the model that has been identified using quite small amount of data. It is important de recall the two following facts: \n\n1. The train/test split has been performed **without shuffle** with $\\texttt{test\\_size}=0.4$ meaning that the first 60% of the scenario is used for the train while the remaining 40% is used to test.\n\n2. The following figures show the four different scenarios used: \n\n:::{.panel-tabset}\n\n### Experiment #1\n\n```{=html}\n{{< include html_files/twin_plot_1.html >}}\n```\n\n### Experiment #2\n\n```{=html}\n{{< include html_files/twin_plot_2.html >}}\n```\n\n### Experiment #3\n\n```{=html}\n{{< include html_files/twin_plot_3.html >}}\n```\n\n### Experiment #4\n\n```{=html}\n{{< include html_files/twin_plot_4.html >}}\n```\n:::\n\n:::{.callout-warning title='Train & test among experiments'}\nIt comes from the observation of the length of each of the four scenarios that the training data concerns almost only the first two experiments while the last two experiments are part of the test data. \n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"html-math-method":"mathjax","css":["styles/styles.css"],"toc":true,"output-file":"rlars_twin.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","bibliography":["biblio.bib"],"resources":["images/sparsity_osc_*.png","images/anomalies_detection_pbstat_*.png","images/sources_of_anomalies_*.png","images/zema_*.png"],"theme":"flatly","link-citations":true,"csl":"styles/apa.csl","title":"rlars","subtitle":"Illustrative example on the twin-pendulum system"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}